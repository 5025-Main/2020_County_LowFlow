{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the 2020 Low Flow Data Processing Script.\n",
    "***\n",
    "#### click in cell and then alt+Enter to execute\n",
    "#### Follow through the steps below to produce wonderfully accurate and high quality flow data\n",
    "#### Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jt -t onedork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Data Directories (will hopefully replace with a database soon)\n",
    "maindir = 'C:/Users/alex.messina/Documents/GitHub/2020_County_LowFlow/'\n",
    "import os\n",
    "os.chdir('C:/Users/alex.messina/Documents/GitHub/2020_County_LowFlow/Python_code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <style>    div#notebook-container    { width: 95%; }    div#menubar-container    \n",
       "{ width: 80%; }    div#maintoolbar-container { width: 99%; } </style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported. Let's go!\n"
     ]
    }
   ],
   "source": [
    "## Import Standard modules\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import string\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import calendar\n",
    "from scipy import signal\n",
    "## Plotting modules\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "## Jupyter interactive plots\n",
    "import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "## for Zentra API\n",
    "from pytz import timezone\n",
    "import json\n",
    "import urllib2\n",
    "## for OneRain data getter \n",
    "import requests\n",
    "import time\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "## Get google sheets\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from df2gspread import df2gspread as d2g \n",
    "## Image tools\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "## Path to Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# Import Custom Modules\n",
    "from ZentraAPI import *\n",
    "from Get_GoogleSheets import *\n",
    "from OneRain_data import *\n",
    "from Excel_Plots import Excel_Plots    \n",
    "from OvertoppingFlows import *\n",
    "from hover_points import *\n",
    "# make the screen bigger!\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(data=\"\"\" <style>    div#notebook-container    { width: 95%; }    div#menubar-container    \n",
    "{ width: 80%; }    div#maintoolbar-container { width: 99%; } </style> \"\"\"))       \n",
    "## Display outputs side by side\n",
    "CSS = \"\"\".output {flex-direction: row;}\"\"\"\n",
    "HTML('<style>{}</style>'.format(CSS))\n",
    "## Open HvF table - can be Google Sheets or straight from GitHub?\n",
    "#HvF = pd.read_csv(maindir+'Ancillary_files/HvF-90degweir.csv',index_col='Level (in)') # Local file\n",
    "#Hvf = open_HvF_90degweir()  # Google Sheets\n",
    "HvF = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/HvF-90degweir.csv',index_col=0) # GitHub\n",
    "## WEIR DIMENSIONS FOR OVERTOPPING FLOWS - can be Google Sheets or straight from GitHub?\n",
    "#weir_dims = pd.read_excel(maindir+'Ancillary_files/Weir Dims 2020.xlsx',sheetname='2020',index_col='Site',skiprows=1, parse_cols='A:I',na_values=['Not Applicable']) # Local file\n",
    "#weir_dims = open_weir_dims() # Google Sheets\n",
    "weir_dims = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Weir_Dims_2020.csv',index_col=0,na_values=['Not Applicable']) # GitHub\n",
    "\n",
    "print \"All modules imported. Let's go!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define site name and start/end times\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Update the Site Name and start/end times\n",
    "site_name = 'SWT-235'\n",
    "start_time_loc = dt.datetime(2020,5,1,0,0)\n",
    "end_time_loc = dt.datetime(2020,5,25,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Level Data\n",
    "***\n",
    "#### from Local file or GitHub\n",
    "#### original data is batch downloaded from Zentra via the ZentraAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-870daeee8589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m## If data is already downloaded, load from csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#WL = pd.DataFrame.from_csv(maindir+'Water_Level_data/'+site_name+'raw_data_ZentraAPI.csv') # Local file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mWL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Water_Level_data/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msite_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_raw_data_ZentraAPI.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# GitHub\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m## Inspect the data from Zentra to make sure it looks right...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mWL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_time_loc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_time_loc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m   1229\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1230\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1231\u001b[1;33m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    373\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m    374\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[0;32m    376\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[0;32m    377\u001b[0m                            else compression)\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\io\\common.pyc\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'infer'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Content-Encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 550\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spyder-master\\python-2.7.10.amd64\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "## FROM CSV\n",
    "## If data is already downloaded, load from csv\n",
    "#WL = pd.DataFrame.from_csv(maindir+'Water_Level_data/'+site_name+'raw_data_ZentraAPI.csv') # Local file\n",
    "WL = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Water_Level_data/'+site_name+'_raw_data_ZentraAPI.csv') # GitHub\n",
    "## Inspect the data from Zentra to make sure it looks right...\n",
    "WL.ix[start_time_loc:end_time_loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Rain data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get the rain gauge used for the site\n",
    "#rain_gauge_site_list = pd.DataFrame.from_csv(maindir+'Ancillary_files/Rain_gauge_to_sites_list.csv') # Local File\n",
    "rain_gauge_site_list = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Rain_gauge_to_sites_list.csv',index_col=0) # GitHub\n",
    "rain_gauge_name = rain_gauge_site_list.ix[site_name]['rain_gauge_name']\n",
    "print 'Rain gauge used for '+site_name+' is '+ rain_gauge_name\n",
    "## Rain gauges indexed by rain gauge name\n",
    "#Rain_gauge_info = pd.DataFrame.from_csv(maindir+'Ancillary_files/Rain_gauge_info.csv') # Local File\n",
    "Rain_gauge_info = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Rain_gauge_info.csv',index_col=0) # GitHub\n",
    "start_date, end_date = start_time_loc.strftime('%Y-%m-%d'), end_time_loc.strftime('%Y-%m-%d')\n",
    "#Rain1D = pd.DataFrame.from_csv(maindir+'Rain_data/'+rain_gauge_name+'_daily_'+start_date+'-'+end_date+'.csv') # Local file\n",
    "\n",
    "Rain1D = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Rain_data/'+rain_gauge_name.replace(' ','%20')+'_daily.csv') # GitHub\n",
    "## Check output \n",
    "Rain1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_gauge_name.replace(' ','%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offsetting/Calibrating Water Level Data\n",
    "***\n",
    "1. Apply special offsets and clip bad data (storms will be clipped later)\n",
    "2. Calculate and apply offset from calibration points \n",
    "3. global offset to manually adjust\n",
    "#### >>> FINAL OFFSET saved to Google Sheets\n",
    "####  \n",
    "### 1a. Special Offsets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add column of zero for data offset\n",
    "WL['spec_offset'] = 0.\n",
    "## Special Offsets and Bad Data Clips, Global Offset from Google Sheets\n",
    "spec_offsets  = open_2020_ClipsOffsets()[0]\n",
    "## Get offsets for each site\n",
    "offsets_list_for_site = spec_offsets[spec_offsets.index  == site_name]\n",
    "offsets_list_for_site\n",
    "## THIS IS AS TUPLES SO THE TUPLE IS INDEXED BY NUMBER NOT STRING\n",
    "for spec_offset in offsets_list_for_site.itertuples():\n",
    "    print ('Special offsets from Google sheet: ')\n",
    "    #print offset\n",
    "    ## set data in bad_data indices to nan\n",
    "    if pd.notnull(spec_offset.Start)==True and pd.notnull(spec_offset.End)==True:\n",
    "        print ('Special offset: '+spec_offset.Start.strftime('%m/%d/%y %H:%M')+' - '+spec_offset.End.strftime('%m/%d/%y %H:%M')+' = '+str(spec_offset.SpecialOffset_in)+ ' inches')\n",
    "        ## insert each offset value to Offset column\n",
    "        WL.loc[spec_offset.Start:spec_offset.End, ['spec_offset']] = spec_offset.SpecialOffset_in\n",
    "    else:\n",
    "        pass\n",
    "    print ('')   \n",
    "## Apply all offsets for unique shifts due to bad data or other issues\n",
    "WL['Level_spec_off'] = WL['in Water Level'] + WL['spec_offset']\n",
    "WL['Level_spec_off'] = WL['Level_spec_off'].round(2)\n",
    "WL[['in Water Level','spec_offset','Level_spec_off']].head()\n",
    "WL[['in Water Level','Level_spec_off']].plot(figsize=(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Clip bad data (don't want it used with Field Measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips =  open_2020_ClipsOffsets()[2]\n",
    "\n",
    "try: # one entry dataframes are weird\n",
    "    clips_for_site = pd.DataFrame(clips.loc[site_name,:])\n",
    "    bad_data_clips = clips_for_site[clips_for_site['Reason']=='Invalid']\n",
    "except:\n",
    "    try:\n",
    "        clips_for_site = pd.DataFrame(clips.loc[site_name,:]).T # have to make DF and Transpose it \n",
    "        bad_data_clips = clips_for_site[clips_for_site['Reason']=='Invalid']   \n",
    "    except KeyError:\n",
    "        print 'No clips found'\n",
    "        bad_data_clips = pd.DataFrame()\n",
    "    \n",
    "## iterate over list of bad data and clip from 'offset_flow_clipped'....\n",
    "print ('Clipping bad/invalid data....')\n",
    "WL['Level_spec_off_clipbad'] = WL['Level_spec_off']\n",
    "for clip in bad_data_clips.iterrows():\n",
    "    clip_start, clip_end = clip[1]['Start'], clip[1]['End']\n",
    "    if pd.isnull(clip_start)==False and pd.isnull(clip_end) == False:\n",
    "        print ('Clipped Invalid data from: '+clip_start.strftime('%m/%d/%y %H:%M')+'-'+clip_end.strftime('%m/%d/%y %H:%M'))\n",
    "        ## set data in WL indices to nan\n",
    "        WL.loc[clip_start:clip_end, ['Level_spec_off_clipbad']] = np.nan\n",
    "    else:\n",
    "        print ('No data to clip...')\n",
    "        pass   \n",
    "WL[['in Water Level','Level_spec_off','Level_spec_off_clipbad']].plot(figsize=(18,6))\n",
    "bad_data_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate offset from calibration measurements\n",
    "### Field Data for Calibrations\n",
    "### Load from Google Sheets or csv if already saved\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Open FDS from Google Sheets\n",
    "fds = open_fds_from_google_sheet()\n",
    "\n",
    "fds['NOTES'] = fds['NOTES'].replace(np.nan, '', regex=True)\n",
    "\n",
    "## Save to csv\n",
    "fds.to_csv(maindir+'Ancillary_files/FDS/Field_data_sheet_backup.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from csv\n",
    "fds = pd.DataFrame.from_csv(maindir+'Ancillary_files/FDS/Field_data_sheet_backup.csv')\n",
    "## Round to 5Min\n",
    "fds['Datetime'] = pd.to_datetime(fds['Date and Time']).apply(lambda x: dt.datetime(x.year, x.month, x.day, x.hour,5*(x.minute // 5)))\n",
    "## Make Index line up with Excel row numbers for easy reference\n",
    "fds.index+=2\n",
    "## Drop any duplicate rows so it doesn't weight the average \n",
    "fds_len =  len(fds)\n",
    "fds = fds.drop_duplicates(keep='first')\n",
    "fds_len_no_dup = len(fds)\n",
    "diff = fds_len - fds_len_no_dup\n",
    "print ('')\n",
    "print ('Dropped '+str(diff)+' duplicate rows')\n",
    "print ('')\n",
    "## strip commas from volume measurement\n",
    "fds['Flow Measurement, Volume in mL'] = fds['Flow Measurement, Volume in mL'].apply(lambda x: x.replace(',',''))\n",
    "## cm to inches\n",
    "fds['Level_above_V_cm'] = fds['Height above (+) or below (-) v-notch in cm']\n",
    "fds['Level_above_V_in'] = np.round(fds['Level_above_V_cm'] / 2.54, 2)\n",
    "## Flow in cfs: mL to cfs divided by seconds\n",
    "fds['Flow_meas_gpm'] = (fds['Flow Measurement, Volume in mL'].astype('float') / fds['Flow Measurement, Time in Seconds '].astype('float')) * 0.02 #1mL per second is 0.02 gpm\n",
    "fds['Flow_meas_gpm'] = fds['Flow_meas_gpm'].round(3)\n",
    "## Display measurements\n",
    "fds[['Site ID','Datetime','Level_above_V_in','Flow_meas_gpm']].head()\n",
    "fds[['Level_above_V_in','Flow_meas_gpm']].plot.scatter('Level_above_V_in','Flow_meas_gpm',figsize=(8,6))\n",
    "#fds[['Site ID','Datetime','Date and Time','Flow Condition',u'NOTES',u'Site Photo or Video?', u'Height above (+) or below (-) v-notch in cm',u'Flow Measurement, Time in Seconds ',u'Flow Measurement, Volume in mL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Offset from Calibration Points for Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Choose time for selecting points\n",
    "cal_start, cal_end = start_time_loc, end_time_loc\n",
    "# FIELD MEASUREMENTS\n",
    "field_meas_level = fds[fds['Site ID']==site_name][['Datetime','Level_above_V_in']]\n",
    "field_meas_flow = fds[fds['Site ID']==site_name][['Datetime','Flow_meas_gpm']]\n",
    "## Add PT level data to field measured level \n",
    "for t in field_meas_level['Datetime'].values:\n",
    "    t = pd.to_datetime(t)\n",
    "    print ('Field measurement time:' + str(t))\n",
    "    try:\n",
    "        print 'Level data from Meter: '\n",
    "        print WL.loc[t]['Level_spec_off_clipbad'] ## water level after manual offset\n",
    "        field_meas_level.loc[field_meas_level['Datetime']==t, 'Level_spec_off_clipbad'] = WL.loc[t]['Level_spec_off_clipbad']\n",
    "    except:\n",
    "        try:\n",
    "            ' Shifting calibration time back 5 miutes....'\n",
    "            t = t - dt.timedelta(minutes=5)\n",
    "            field_meas_level.loc[field_meas_level['Datetime']==t, 'Level_in'] = WL.loc[t]['Level_in']\n",
    "        except:\n",
    "            pass\n",
    "## Add the flow that would be predicted from v-notch equation\n",
    "try: \n",
    "    field_meas_level.loc[:,'Predicted_flow'] =  [HvF.loc[np.round(x,2)]['Q (GPM)'] for x in field_meas_level['Level_above_V_in'].values]\n",
    "except KeyError:\n",
    "    #field_meas_level_nozeros = field_meas_level[field_meas_level['Level_above_V_in'] >=0.]\n",
    "    #field_meas_level_nozeros.loc['Predicted_flow'] = 0.\n",
    "    field_meas_level.loc[:,'Predicted_flow'] = 0.\n",
    "## Display field measurements of level and flow \n",
    "display(field_meas_level, field_meas_flow)\n",
    "## Calibration measurements made in May, QC measurements made June-Sept\n",
    "field_meas_level_QC = field_meas_level[field_meas_level['Datetime']>cal_end] ## after 5/31 should be only QC measurements\n",
    "field_meas_level_Cal = field_meas_level[(field_meas_level['Datetime']>=cal_start) & (field_meas_level['Datetime']<=cal_end)]\n",
    "field_meas_flow_QC =  field_meas_flow[field_meas_flow['Datetime']>cal_end] ## after 5/31 should be only QC measurements\n",
    "field_meas_flow_Cal =  field_meas_flow[(field_meas_flow['Datetime']>=cal_start) & (field_meas_flow['Datetime']<=cal_end)]\n",
    "## Calculate average offset from field data\n",
    "field_meas_level_Cal['calculated offset'] = field_meas_level_Cal['Level_above_V_in'] - field_meas_level_Cal['Level_spec_off_clipbad'] ## Measured - Manually adjusted PT reading\n",
    "## Calculate total offset  \n",
    "calculated_offset = field_meas_level_Cal['calculated offset'].mean()\n",
    "print 'Calculated offset = '+str(calculated_offset)+' in'\n",
    "## Copy over data that has already had special offsets applied and bad data clipped\n",
    "## Apply calculated offset\n",
    "WL['Level_spec_off_clipbad_calc_off'] = WL['Level_spec_off_clipbad'] + calculated_offset\n",
    "## Display water level data\n",
    "WL[['in Water Level','Level_spec_off','Level_spec_off_clipbad','Level_spec_off_clipbad_calc_off']].plot(figsize=(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Global Offset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get global offset (stored in Google Sheets)\n",
    "glob_offsets = open_2020_ClipsOffsets()[1]\n",
    "glob_offset = glob_offsets.ix[site_name]['GlobalOffset_in']\n",
    "print 'Global offset for '+site_name+' = '+str(glob_offset) +' in.'\n",
    "## Apply global offset\n",
    "WL['Level_spec_off_clipbad_calc_off_glob_off'] = WL['Level_spec_off_clipbad_calc_off'] + glob_offset\n",
    "## Final, offset and cleaned water level data for flow calculation\n",
    "WL['Level_in'] = WL['Level_spec_off_clipbad_calc_off_glob_off']\n",
    "## Highlight missing data  \n",
    "missing_data = pd.DataFrame(WL[np.isnan(WL['Level_in'])]['Level_in'])\n",
    "missing_data['missing_level_data'] = 0.\n",
    "missing_data = missing_data.reindex(index=pd.date_range(start_time_loc,end_time_loc,freq='5Min'))\n",
    "## Display water level data\n",
    "ax = WL[['in Water Level','Level_spec_off','Level_spec_off_clipbad','Level_spec_off_clipbad_calc_off','Level_spec_off_clipbad_calc_off_glob_off']].plot(figsize=(18,6))\n",
    "ax.axhline(0,c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Total offset\n",
    "#### >>> Saved in Google Sheets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOffsets = open_2020_FinalOffsets()\n",
    "FinalOffsets = FinalOffsets[['CalculatedOffset_in','GlobalOffset_in','FinalOffset_in']]\n",
    "FinalOffsets.loc[site_name,'CalculatedOffset_in'] = calculated_offset\n",
    "FinalOffsets.loc[site_name,'GlobalOffset_in'] = glob_offset\n",
    "FinalOffset = calculated_offset + glob_offset\n",
    "FinalOffsets.loc[site_name,'FinalOffset_in'] = FinalOffset\n",
    "save_df_to_GoogleSheets(FinalOffsets, worksheet_name='FinalOffsets',spreadsheet_key=\"1U0UnBJrpMNEtDYctO2GW0fuobdc8vJfdLIWbvSr--ss\")\n",
    "FinalOffsets.to_csv(maindir+'Ancillary_files/FinalOffsets_backup.csv')\n",
    "## Display\n",
    "FinalOffsets.ix[site_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Flow\n",
    "### HvF table for 90deg v-notch until water level exceeds v, then CTRSC equation\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HvF table for 90 deg v-notch\n",
    "def level_to_gpm_vnotch(x):\n",
    "    if x < 0:\n",
    "        gpm = 0.0\n",
    "    else:\n",
    "        gpm = HvF.loc[np.round(x,2)]['Q (GPM)']\n",
    "    return gpm\n",
    "WL['Flow_gpm_v'] = WL['Level_in'].apply(lambda x: level_to_gpm_vnotch(x))    \n",
    "## Calculate flows when overtopping the weir\n",
    "WL['Flow_gpm'] = CTRSC_compound_weir(site_name, WL, weir_dims)#,  True, True)\n",
    "WL[['Level_in','Flow_gpm']].plot(figsize=(18,8),grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC Hydrograph and Clip storm flow\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clip storm flows\n",
    "clips =  open_2020_ClipsOffsets()[2] # reload every time so Google sheet can be updated\n",
    "try: # one entry dataframes are weird\n",
    "    clips_for_site = pd.DataFrame(clips.loc[site_name,:])\n",
    "    storm_clips = clips_for_site[clips_for_site['Reason']=='Storm']\n",
    "except:\n",
    "    try:\n",
    "        clips_for_site = pd.DataFrame(clips.loc[site_name,:]).T # have to make DF and Transpose it \n",
    "        storm_clips = clips_for_site[clips_for_site['Reason']=='Storm'] \n",
    "    except KeyError:\n",
    "        storm_clips = pd.DataFrame()\n",
    "## iterate over list of bad data and clip from 'Flow_gpm'....\n",
    "print ('Clipping stormflow data....')\n",
    "WL['Flow_gpm_storm_clipped'] = WL['Flow_gpm']\n",
    "for clip in storm_clips.iterrows():\n",
    "    clip_start, clip_end = clip[1]['Start'], clip[1]['End']\n",
    "    if pd.isnull(clip_start)==False and pd.isnull(clip_end) == False:\n",
    "        print ('Clipped storm data from: '+clip_start.strftime('%m/%d/%y %H:%M')+'-'+clip_end.strftime('%m/%d/%y %H:%M'))\n",
    "        ## set data in WL indices to nan\n",
    "        WL.loc[clip_start:clip_end, ['Flow_gpm_storm_clipped']] = np.nan\n",
    "    else:\n",
    "        print ('No data to clip...')\n",
    "        pass   \n",
    "storm_clips\n",
    "\n",
    "########################################\n",
    "##### QC Hydrograph ###################\n",
    "#####################################\n",
    "fig, (ax1, ax2, ax4) = plt.subplots(3,1,figsize=(18,10),sharex=True)\n",
    "## Plot full scale level data\n",
    "ax1.plot_date(missing_data.index,missing_data['missing_level_data'],marker='None',ls='-',c='r',label='Missing Data')\n",
    "## raw\n",
    "ax1.plot_date(WL.index, WL['in Water Level'], marker='None',ls='-',c='grey',alpha=0.25,label='Raw level data')\n",
    "##raw +final offset\n",
    "ax1.plot_date(WL.index, WL['Level_in'], marker='None',ls='-',c='g',alpha=0.25,label='Raw level + FinalOffset ('+\"%.2f\"%FinalOffset+' in.)')\n",
    "## Plot field measurements\n",
    "ax1.plot_date(field_meas_level_Cal['Datetime'].values,field_meas_level_Cal['Level_above_V_in'].values,marker='s',c='b',label='Initial Calibration measurements')\n",
    "ax1.plot_date(field_meas_level_QC['Datetime'].values,field_meas_level_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k')\n",
    "textstr = 'Weir crest height: '+str(weir_dims.loc[site_name,'h2'])+' inches'\n",
    "#ax1.annotate(textstr, (mpl.dates.date2num(pd.to_datetime(weir_dims.loc['CAR-007','Measurement date'])),weir_dims.loc[site_name,'h2']))\n",
    "## Plot temp\n",
    "#ax1_1 = ax1.twinx()\n",
    "#ax1_1.plot_date(WL.index, WL['Temp_F'], marker='None',ls='-',c='grey',label='Temp F')\n",
    "\n",
    "## Plot full scale flow data or\n",
    "## Conductivity data if available\n",
    "if u'mS/cm EC' in WL.columns:\n",
    "    ax2.plot_date(WL.index, WL[u'mS/cm EC'], marker='None',ls='-',c='orange',label= 'mS/cm EC')\n",
    "    ax2.set_ylabel('Sp.Cond (mS/cm)',color='orange'), \n",
    "else:\n",
    "    ax2.plot_date(WL.index, WL['Flow_gpm'], marker='None',ls='-',c='teal',label='Flow (gpm)')\n",
    "    ax2.set_ylabel('Flow (gpm)',color='b'), \n",
    "## Put notes on the plot\n",
    "for row in fds[fds['Site ID']==site_name][['Datetime','NOTES']].iterrows():\n",
    "    note = '\\n'.join(textwrap.wrap(str(row[1]['NOTES']), 16))\n",
    "    #ax2.annotate(note,xy=(pd.to_datetime(row[1]['Datetime']),WL['Flow_gpm'].mean()),rotation=90,verticalalignment='bottom')\n",
    "    ax2.axvline(pd.to_datetime(row[1]['Datetime']),color='grey',alpha=0.5)\n",
    "### Plot precip on inverted, secondary y axis\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot_date(Rain1D.index, Rain1D['Rain_in'], marker='None',ls='steps-mid',color='teal',label='Precip: '+rain_gauge_name)\n",
    "## Plot flow data, compound weir\n",
    "ax4.plot_date(WL.index, WL['Flow_gpm'], marker='None',ls='-',c='grey',alpha=0.5,label='Flow (gpm)')\n",
    "## Plot flow data, storms clipped\n",
    "ax4.plot_date(WL.index, WL['Flow_gpm_storm_clipped'], marker='None',ls='-',c='green',label='Flow(gpm), storms clipped')\n",
    "## Plot calibration field measurements\n",
    "ax4.plot_date(field_meas_flow_Cal['Datetime'].values,field_meas_flow_Cal['Flow_meas_gpm'].values,marker='o',c='b',label='Initial Calibration measurements')\n",
    "## Plot QC field measurements\n",
    "ax4.plot_date(field_meas_flow_QC['Datetime'].values,field_meas_flow_QC['Flow_meas_gpm'].values,marker='o',c='r',label='Follow-up QC measurements')\n",
    "\n",
    "## Previous deliverable data\n",
    "#ax4.plot_date(del_df.index,del_df['Flow compound weir (gpm)'], marker='None',ls='-',c='b',label='Previous deliverable')\n",
    "\n",
    "### Plot precip on inverted, secondary y axis\n",
    "ax4_2 = ax4.twinx()\n",
    "ax4_2.plot_date(Rain1D.index, Rain1D['Rain_in'], marker='None',ls='steps-mid',color='teal',label='Precip: '+rain_gauge_name)\n",
    "\n",
    "## Format/set limits\n",
    "## full scale flow\n",
    "ax1.set_ylim(-3, WL['Level_in'].max() * 1.1)\n",
    "#ax2.set_ylim(-WL['offset_flow'].max() * 0.5, WL['offset_flow'].max() * 2.)\n",
    "ax3.set_ylim(0, Rain1D['Rain_in'].max() * 2.)\n",
    "ax4_2.set_ylim(0, Rain1D['Rain_in'].max() * 3.)\n",
    "ax3.invert_yaxis(), ax4_2.invert_yaxis()\n",
    "\n",
    "## low flow\n",
    "ax4.set_ylim(-WL['Flow_gpm_storm_clipped'].max() * 0.45, WL['Flow_gpm_storm_clipped'].max() * 1.2)\n",
    "## set x-axis to monitoring period\n",
    "ax1.set_xlim(start_time_loc, end_time_loc)\n",
    "ax1.grid(True)\n",
    "ax1.set_ylabel('Level (inches)',color='g')\n",
    "ax3.set_ylabel('Precip (inches)',color='teal')\n",
    "ax4.set_ylabel('Flow (gpm)',color='b')\n",
    "\n",
    "ax1.legend(fontsize=12,numpoints=1,ncol=1,loc='upper left')\n",
    "ax2.legend(fontsize=12,loc='lower left'), ax3.legend(fontsize=12,loc='lower right')\n",
    "ax4.legend(fontsize=12,numpoints=1,ncol=5,loc='lower left')\n",
    "\n",
    "ax4.xaxis.set_major_formatter(mpl.dates.DateFormatter('%A \\n %m/%d/%y %H:%M'))\n",
    "\n",
    "fig.suptitle('Data processing for site: '+site_name,fontsize=16,fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
