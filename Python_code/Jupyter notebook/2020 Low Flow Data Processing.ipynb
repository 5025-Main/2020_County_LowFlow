{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the 2020 Low Flow Data Processing Script.\n",
    "***\n",
    "#### click in cell and then alt+Enter to execute\n",
    "#### Follow through the steps below to produce wonderfully accurate and high quality flow data\n",
    "#### Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jt -t onedork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Data Directories (will hopefully replace with a database soon)\n",
    "maindir = 'C:/Users/alex.messina/Documents/GitHub/2020_County_LowFlow/'\n",
    "import os\n",
    "os.chdir('C:/Users/alex.messina/Documents/GitHub/2020_County_LowFlow/Python_code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       " <style>    div#notebook-container    { width: 95%; }    div#menubar-container    \n",
       "{ width: 80%; }    div#maintoolbar-container { width: 99%; } </style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported. Let's go!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "## Import Standard modules\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import string\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import calendar\n",
    "from scipy import signal\n",
    "## Plotting modules\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "## Jupyter interactive plots\n",
    "import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "## for Zentra API\n",
    "from pytz import timezone\n",
    "import json\n",
    "import urllib2\n",
    "## for OneRain data getter \n",
    "import requests\n",
    "import time\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "## Get google sheets\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from df2gspread import df2gspread as d2g \n",
    "## Image tools\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "## Path to Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# Import Custom Modules\n",
    "from ZentraAPI import *\n",
    "from Get_GoogleSheets import *\n",
    "from OneRain_data import *\n",
    "from Excel_Plots import *\n",
    "from OvertoppingFlows import *\n",
    "from hover_points import *\n",
    "# make the screen bigger!\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(data=\"\"\" <style>    div#notebook-container    { width: 95%; }    div#menubar-container    \n",
    "{ width: 80%; }    div#maintoolbar-container { width: 99%; } </style> \"\"\"))       \n",
    "## Display outputs side by side\n",
    "CSS = \"\"\".output {flex-direction: row;}\"\"\"\n",
    "HTML('<style>{}</style>'.format(CSS))\n",
    "## Open HvF table - can be Google Sheets or straight from GitHub?\n",
    "#HvF = pd.read_csv(maindir+'Ancillary_files/HvF-90degweir.csv',index_col='Level (in)') # Local file\n",
    "#Hvf = open_HvF_90degweir()  # Google Sheets\n",
    "HvF = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/HvF-90degweir.csv',index_col=0) # GitHub\n",
    "## WEIR DIMENSIONS FOR OVERTOPPING FLOWS - can be Google Sheets or straight from GitHub?\n",
    "#weir_dims = pd.read_excel(maindir+'Ancillary_files/Weir Dims 2020.xlsx',sheetname='2020',index_col='Site',skiprows=1, parse_cols='A:I',na_values=['Not Applicable']) # Local file\n",
    "#weir_dims = open_weir_dims() # Google Sheets\n",
    "weir_dims = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Weir_Dims_2020.csv',index_col=0,na_values=['Not Applicable']) # GitHub\n",
    "\n",
    "print \"All modules imported. Let's go!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define site name and start/end times\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Update the Site Name and start/end times\n",
    "site_name = 'SDR-041'\n",
    "start_time_loc = dt.datetime(2020,5,1,0,0)\n",
    "end_time_loc = dt.datetime(2020,6,1,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Level Data\n",
    "***\n",
    "#### from Local file or GitHub\n",
    "#### original data is batch downloaded from Zentra via the ZentraAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in Water Level</th>\n",
       "      <th>°F Water Temperature</th>\n",
       "      <th>mS/cm EC</th>\n",
       "      <th>Sensor Metadata</th>\n",
       "      <th>% Battery Percent</th>\n",
       "      <th>mV Battery Voltage</th>\n",
       "      <th>kPa Reference Pressure</th>\n",
       "      <th>°F Logger Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:00:00</th>\n",
       "      <td>5.78739</td>\n",
       "      <td>68.900</td>\n",
       "      <td>1.678804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7571</td>\n",
       "      <td>99.67</td>\n",
       "      <td>66.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:05:00</th>\n",
       "      <td>5.82676</td>\n",
       "      <td>68.900</td>\n",
       "      <td>1.599558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99</td>\n",
       "      <td>7481</td>\n",
       "      <td>99.67</td>\n",
       "      <td>66.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:10:00</th>\n",
       "      <td>5.78739</td>\n",
       "      <td>68.900</td>\n",
       "      <td>1.534617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98</td>\n",
       "      <td>7470</td>\n",
       "      <td>99.66</td>\n",
       "      <td>66.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:15:00</th>\n",
       "      <td>6.02361</td>\n",
       "      <td>68.828</td>\n",
       "      <td>1.291219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98</td>\n",
       "      <td>7454</td>\n",
       "      <td>99.66</td>\n",
       "      <td>66.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:20:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.370882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7520</td>\n",
       "      <td>99.66</td>\n",
       "      <td>66.452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:25:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.358313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7525</td>\n",
       "      <td>99.65</td>\n",
       "      <td>66.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:30:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.367729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7530</td>\n",
       "      <td>99.66</td>\n",
       "      <td>65.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:35:00</th>\n",
       "      <td>5.70865</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.409289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7531</td>\n",
       "      <td>99.66</td>\n",
       "      <td>65.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:40:00</th>\n",
       "      <td>5.70865</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.435489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7532</td>\n",
       "      <td>99.66</td>\n",
       "      <td>65.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:45:00</th>\n",
       "      <td>5.74802</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.472313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7527</td>\n",
       "      <td>99.66</td>\n",
       "      <td>65.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:50:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.552387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7531</td>\n",
       "      <td>99.66</td>\n",
       "      <td>65.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:55:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.445440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7533</td>\n",
       "      <td>99.66</td>\n",
       "      <td>64.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:00:00</th>\n",
       "      <td>5.66928</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.367729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:05:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.389953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7535</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:10:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.374042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:15:00</th>\n",
       "      <td>5.70865</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.358313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:20:00</th>\n",
       "      <td>5.66928</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.442115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:25:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.576</td>\n",
       "      <td>1.432188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:30:00</th>\n",
       "      <td>5.70865</td>\n",
       "      <td>68.540</td>\n",
       "      <td>1.380384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:35:00</th>\n",
       "      <td>5.66928</td>\n",
       "      <td>68.540</td>\n",
       "      <td>1.396368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7534</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:40:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.540</td>\n",
       "      <td>1.315225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7536</td>\n",
       "      <td>99.67</td>\n",
       "      <td>64.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:45:00</th>\n",
       "      <td>5.59054</td>\n",
       "      <td>68.540</td>\n",
       "      <td>1.348963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7539</td>\n",
       "      <td>99.67</td>\n",
       "      <td>63.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:50:00</th>\n",
       "      <td>5.59054</td>\n",
       "      <td>68.576</td>\n",
       "      <td>1.345860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7539</td>\n",
       "      <td>99.66</td>\n",
       "      <td>63.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 01:55:00</th>\n",
       "      <td>5.62991</td>\n",
       "      <td>68.684</td>\n",
       "      <td>1.285287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7539</td>\n",
       "      <td>99.66</td>\n",
       "      <td>63.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 02:00:00</th>\n",
       "      <td>5.66928</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.309182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7541</td>\n",
       "      <td>99.66</td>\n",
       "      <td>63.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 02:05:00</th>\n",
       "      <td>5.98424</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.406048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7540</td>\n",
       "      <td>99.66</td>\n",
       "      <td>64.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 02:10:00</th>\n",
       "      <td>5.90550</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.479108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7541</td>\n",
       "      <td>99.66</td>\n",
       "      <td>64.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 02:15:00</th>\n",
       "      <td>5.94487</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.377209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7541</td>\n",
       "      <td>99.65</td>\n",
       "      <td>64.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 02:20:00</th>\n",
       "      <td>6.02361</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.419058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7538</td>\n",
       "      <td>99.65</td>\n",
       "      <td>64.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 02:25:00</th>\n",
       "      <td>6.10235</td>\n",
       "      <td>68.720</td>\n",
       "      <td>1.435489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7539</td>\n",
       "      <td>99.64</td>\n",
       "      <td>64.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 21:35:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.520</td>\n",
       "      <td>1.648162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.75</td>\n",
       "      <td>67.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 21:40:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.520</td>\n",
       "      <td>1.644372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7265</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 21:45:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.484</td>\n",
       "      <td>1.636817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7264</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 21:50:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.584893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 21:55:00</th>\n",
       "      <td>5.31495</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.588547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7265</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:00:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:05:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.584893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:10:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7264</td>\n",
       "      <td>99.76</td>\n",
       "      <td>67.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:15:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.76</td>\n",
       "      <td>66.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:20:00</th>\n",
       "      <td>5.23621</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.584893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7264</td>\n",
       "      <td>99.76</td>\n",
       "      <td>66.848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:25:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.584893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.76</td>\n",
       "      <td>66.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:30:00</th>\n",
       "      <td>5.31495</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7268</td>\n",
       "      <td>99.76</td>\n",
       "      <td>66.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:35:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.76</td>\n",
       "      <td>66.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:40:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.76</td>\n",
       "      <td>66.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:45:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7269</td>\n",
       "      <td>99.75</td>\n",
       "      <td>66.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:50:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7269</td>\n",
       "      <td>99.75</td>\n",
       "      <td>66.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 22:55:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.340</td>\n",
       "      <td>1.581248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.75</td>\n",
       "      <td>66.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:00:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.232</td>\n",
       "      <td>1.577611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.75</td>\n",
       "      <td>65.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:05:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.577611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7268</td>\n",
       "      <td>99.75</td>\n",
       "      <td>65.768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:10:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.573983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.75</td>\n",
       "      <td>65.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:15:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.573983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.75</td>\n",
       "      <td>65.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:20:00</th>\n",
       "      <td>5.31495</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.577611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.74</td>\n",
       "      <td>65.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:25:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.573983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7266</td>\n",
       "      <td>99.74</td>\n",
       "      <td>65.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:30:00</th>\n",
       "      <td>5.31495</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.573983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7268</td>\n",
       "      <td>99.73</td>\n",
       "      <td>65.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:35:00</th>\n",
       "      <td>5.31495</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.573983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.72</td>\n",
       "      <td>65.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:40:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.570363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7268</td>\n",
       "      <td>99.72</td>\n",
       "      <td>65.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:45:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.573983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7269</td>\n",
       "      <td>99.72</td>\n",
       "      <td>64.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:50:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.570363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7268</td>\n",
       "      <td>99.73</td>\n",
       "      <td>64.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31 23:55:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.570363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7267</td>\n",
       "      <td>99.73</td>\n",
       "      <td>64.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01 00:00:00</th>\n",
       "      <td>5.27558</td>\n",
       "      <td>70.160</td>\n",
       "      <td>1.566751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90</td>\n",
       "      <td>7270</td>\n",
       "      <td>99.72</td>\n",
       "      <td>64.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8929 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     in Water Level  °F Water Temperature  mS/cm EC  \\\n",
       "2020-05-01 00:00:00         5.78739                68.900  1.678804   \n",
       "2020-05-01 00:05:00         5.82676                68.900  1.599558   \n",
       "2020-05-01 00:10:00         5.78739                68.900  1.534617   \n",
       "2020-05-01 00:15:00         6.02361                68.828  1.291219   \n",
       "2020-05-01 00:20:00         5.62991                68.720  1.370882   \n",
       "2020-05-01 00:25:00         5.62991                68.720  1.358313   \n",
       "2020-05-01 00:30:00         5.62991                68.720  1.367729   \n",
       "2020-05-01 00:35:00         5.70865                68.720  1.409289   \n",
       "2020-05-01 00:40:00         5.70865                68.720  1.435489   \n",
       "2020-05-01 00:45:00         5.74802                68.720  1.472313   \n",
       "2020-05-01 00:50:00         5.62991                68.720  1.552387   \n",
       "2020-05-01 00:55:00         5.62991                68.720  1.445440   \n",
       "2020-05-01 01:00:00         5.66928                68.720  1.367729   \n",
       "2020-05-01 01:05:00         5.62991                68.720  1.389953   \n",
       "2020-05-01 01:10:00         5.62991                68.720  1.374042   \n",
       "2020-05-01 01:15:00         5.70865                68.720  1.358313   \n",
       "2020-05-01 01:20:00         5.66928                68.720  1.442115   \n",
       "2020-05-01 01:25:00         5.62991                68.576  1.432188   \n",
       "2020-05-01 01:30:00         5.70865                68.540  1.380384   \n",
       "2020-05-01 01:35:00         5.66928                68.540  1.396368   \n",
       "2020-05-01 01:40:00         5.62991                68.540  1.315225   \n",
       "2020-05-01 01:45:00         5.59054                68.540  1.348963   \n",
       "2020-05-01 01:50:00         5.59054                68.576  1.345860   \n",
       "2020-05-01 01:55:00         5.62991                68.684  1.285287   \n",
       "2020-05-01 02:00:00         5.66928                68.720  1.309182   \n",
       "2020-05-01 02:05:00         5.98424                68.720  1.406048   \n",
       "2020-05-01 02:10:00         5.90550                68.720  1.479108   \n",
       "2020-05-01 02:15:00         5.94487                68.720  1.377209   \n",
       "2020-05-01 02:20:00         6.02361                68.720  1.419058   \n",
       "2020-05-01 02:25:00         6.10235                68.720  1.435489   \n",
       "...                             ...                   ...       ...   \n",
       "2020-05-31 21:35:00         5.27558                70.520  1.648162   \n",
       "2020-05-31 21:40:00         5.27558                70.520  1.644372   \n",
       "2020-05-31 21:45:00         5.27558                70.484  1.636817   \n",
       "2020-05-31 21:50:00         5.27558                70.340  1.584893   \n",
       "2020-05-31 21:55:00         5.31495                70.340  1.588547   \n",
       "2020-05-31 22:00:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:05:00         5.27558                70.340  1.584893   \n",
       "2020-05-31 22:10:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:15:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:20:00         5.23621                70.340  1.584893   \n",
       "2020-05-31 22:25:00         5.27558                70.340  1.584893   \n",
       "2020-05-31 22:30:00         5.31495                70.340  1.581248   \n",
       "2020-05-31 22:35:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:40:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:45:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:50:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 22:55:00         5.27558                70.340  1.581248   \n",
       "2020-05-31 23:00:00         5.27558                70.232  1.577611   \n",
       "2020-05-31 23:05:00         5.27558                70.160  1.577611   \n",
       "2020-05-31 23:10:00         5.27558                70.160  1.573983   \n",
       "2020-05-31 23:15:00         5.27558                70.160  1.573983   \n",
       "2020-05-31 23:20:00         5.31495                70.160  1.577611   \n",
       "2020-05-31 23:25:00         5.27558                70.160  1.573983   \n",
       "2020-05-31 23:30:00         5.31495                70.160  1.573983   \n",
       "2020-05-31 23:35:00         5.31495                70.160  1.573983   \n",
       "2020-05-31 23:40:00         5.27558                70.160  1.570363   \n",
       "2020-05-31 23:45:00         5.27558                70.160  1.573983   \n",
       "2020-05-31 23:50:00         5.27558                70.160  1.570363   \n",
       "2020-05-31 23:55:00         5.27558                70.160  1.570363   \n",
       "2020-06-01 00:00:00         5.27558                70.160  1.566751   \n",
       "\n",
       "                      Sensor Metadata  % Battery Percent  mV Battery Voltage  \\\n",
       "2020-05-01 00:00:00               0.0                100                7571   \n",
       "2020-05-01 00:05:00               0.0                 99                7481   \n",
       "2020-05-01 00:10:00               0.0                 98                7470   \n",
       "2020-05-01 00:15:00               0.0                 98                7454   \n",
       "2020-05-01 00:20:00               0.0                100                7520   \n",
       "2020-05-01 00:25:00               0.0                100                7525   \n",
       "2020-05-01 00:30:00               0.0                100                7530   \n",
       "2020-05-01 00:35:00               0.0                100                7531   \n",
       "2020-05-01 00:40:00               0.0                100                7532   \n",
       "2020-05-01 00:45:00               0.0                100                7527   \n",
       "2020-05-01 00:50:00               0.0                100                7531   \n",
       "2020-05-01 00:55:00               0.0                100                7533   \n",
       "2020-05-01 01:00:00               0.0                100                7536   \n",
       "2020-05-01 01:05:00               0.0                100                7535   \n",
       "2020-05-01 01:10:00               0.0                100                7536   \n",
       "2020-05-01 01:15:00               0.0                100                7536   \n",
       "2020-05-01 01:20:00               0.0                100                7536   \n",
       "2020-05-01 01:25:00               0.0                100                7536   \n",
       "2020-05-01 01:30:00               0.0                100                7536   \n",
       "2020-05-01 01:35:00               0.0                100                7534   \n",
       "2020-05-01 01:40:00               0.0                100                7536   \n",
       "2020-05-01 01:45:00               0.0                100                7539   \n",
       "2020-05-01 01:50:00               0.0                100                7539   \n",
       "2020-05-01 01:55:00               0.0                100                7539   \n",
       "2020-05-01 02:00:00               0.0                100                7541   \n",
       "2020-05-01 02:05:00               0.0                100                7540   \n",
       "2020-05-01 02:10:00               0.0                100                7541   \n",
       "2020-05-01 02:15:00               0.0                100                7541   \n",
       "2020-05-01 02:20:00               0.0                100                7538   \n",
       "2020-05-01 02:25:00               0.0                100                7539   \n",
       "...                               ...                ...                 ...   \n",
       "2020-05-31 21:35:00               0.0                 90                7267   \n",
       "2020-05-31 21:40:00               0.0                 90                7265   \n",
       "2020-05-31 21:45:00               0.0                 90                7264   \n",
       "2020-05-31 21:50:00               0.0                 90                7267   \n",
       "2020-05-31 21:55:00               0.0                 90                7265   \n",
       "2020-05-31 22:00:00               0.0                 90                7267   \n",
       "2020-05-31 22:05:00               0.0                 90                7267   \n",
       "2020-05-31 22:10:00               0.0                 90                7264   \n",
       "2020-05-31 22:15:00               0.0                 90                7267   \n",
       "2020-05-31 22:20:00               0.0                 90                7264   \n",
       "2020-05-31 22:25:00               0.0                 90                7266   \n",
       "2020-05-31 22:30:00               0.0                 90                7268   \n",
       "2020-05-31 22:35:00               0.0                 90                7266   \n",
       "2020-05-31 22:40:00               0.0                 90                7267   \n",
       "2020-05-31 22:45:00               0.0                 90                7269   \n",
       "2020-05-31 22:50:00               0.0                 90                7269   \n",
       "2020-05-31 22:55:00               0.0                 90                7266   \n",
       "2020-05-31 23:00:00               0.0                 90                7266   \n",
       "2020-05-31 23:05:00               0.0                 90                7268   \n",
       "2020-05-31 23:10:00               0.0                 90                7266   \n",
       "2020-05-31 23:15:00               0.0                 90                7266   \n",
       "2020-05-31 23:20:00               0.0                 90                7266   \n",
       "2020-05-31 23:25:00               0.0                 90                7266   \n",
       "2020-05-31 23:30:00               0.0                 90                7268   \n",
       "2020-05-31 23:35:00               0.0                 90                7267   \n",
       "2020-05-31 23:40:00               0.0                 90                7268   \n",
       "2020-05-31 23:45:00               0.0                 90                7269   \n",
       "2020-05-31 23:50:00               0.0                 90                7268   \n",
       "2020-05-31 23:55:00               0.0                 90                7267   \n",
       "2020-06-01 00:00:00               0.0                 90                7270   \n",
       "\n",
       "                     kPa Reference Pressure  °F Logger Temperature  \n",
       "2020-05-01 00:00:00                   99.67                 66.038  \n",
       "2020-05-01 00:05:00                   99.67                 66.272  \n",
       "2020-05-01 00:10:00                   99.66                 66.578  \n",
       "2020-05-01 00:15:00                   99.66                 66.722  \n",
       "2020-05-01 00:20:00                   99.66                 66.452  \n",
       "2020-05-01 00:25:00                   99.65                 66.056  \n",
       "2020-05-01 00:30:00                   99.66                 65.804  \n",
       "2020-05-01 00:35:00                   99.66                 65.606  \n",
       "2020-05-01 00:40:00                   99.66                 65.444  \n",
       "2020-05-01 00:45:00                   99.66                 65.282  \n",
       "2020-05-01 00:50:00                   99.66                 65.120  \n",
       "2020-05-01 00:55:00                   99.66                 64.976  \n",
       "2020-05-01 01:00:00                   99.67                 64.832  \n",
       "2020-05-01 01:05:00                   99.67                 64.724  \n",
       "2020-05-01 01:10:00                   99.67                 64.598  \n",
       "2020-05-01 01:15:00                   99.67                 64.490  \n",
       "2020-05-01 01:20:00                   99.67                 64.364  \n",
       "2020-05-01 01:25:00                   99.67                 64.256  \n",
       "2020-05-01 01:30:00                   99.67                 64.166  \n",
       "2020-05-01 01:35:00                   99.67                 64.076  \n",
       "2020-05-01 01:40:00                   99.67                 64.004  \n",
       "2020-05-01 01:45:00                   99.67                 63.968  \n",
       "2020-05-01 01:50:00                   99.66                 63.950  \n",
       "2020-05-01 01:55:00                   99.66                 63.968  \n",
       "2020-05-01 02:00:00                   99.66                 63.986  \n",
       "2020-05-01 02:05:00                   99.66                 64.004  \n",
       "2020-05-01 02:10:00                   99.66                 64.022  \n",
       "2020-05-01 02:15:00                   99.65                 64.058  \n",
       "2020-05-01 02:20:00                   99.65                 64.112  \n",
       "2020-05-01 02:25:00                   99.64                 64.166  \n",
       "...                                     ...                    ...  \n",
       "2020-05-31 21:35:00                   99.75                 67.964  \n",
       "2020-05-31 21:40:00                   99.76                 67.838  \n",
       "2020-05-31 21:45:00                   99.76                 67.730  \n",
       "2020-05-31 21:50:00                   99.76                 67.622  \n",
       "2020-05-31 21:55:00                   99.76                 67.496  \n",
       "2020-05-31 22:00:00                   99.76                 67.370  \n",
       "2020-05-31 22:05:00                   99.76                 67.244  \n",
       "2020-05-31 22:10:00                   99.76                 67.100  \n",
       "2020-05-31 22:15:00                   99.76                 66.956  \n",
       "2020-05-31 22:20:00                   99.76                 66.848  \n",
       "2020-05-31 22:25:00                   99.76                 66.758  \n",
       "2020-05-31 22:30:00                   99.76                 66.650  \n",
       "2020-05-31 22:35:00                   99.76                 66.524  \n",
       "2020-05-31 22:40:00                   99.76                 66.380  \n",
       "2020-05-31 22:45:00                   99.75                 66.254  \n",
       "2020-05-31 22:50:00                   99.75                 66.146  \n",
       "2020-05-31 22:55:00                   99.75                 66.038  \n",
       "2020-05-31 23:00:00                   99.75                 65.894  \n",
       "2020-05-31 23:05:00                   99.75                 65.768  \n",
       "2020-05-31 23:10:00                   99.75                 65.678  \n",
       "2020-05-31 23:15:00                   99.75                 65.624  \n",
       "2020-05-31 23:20:00                   99.74                 65.552  \n",
       "2020-05-31 23:25:00                   99.74                 65.462  \n",
       "2020-05-31 23:30:00                   99.73                 65.336  \n",
       "2020-05-31 23:35:00                   99.72                 65.174  \n",
       "2020-05-31 23:40:00                   99.72                 65.048  \n",
       "2020-05-31 23:45:00                   99.72                 64.958  \n",
       "2020-05-31 23:50:00                   99.73                 64.886  \n",
       "2020-05-31 23:55:00                   99.73                 64.778  \n",
       "2020-06-01 00:00:00                   99.72                 64.688  \n",
       "\n",
       "[8929 rows x 8 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## FROM CSV\n",
    "## If data is already downloaded, load from csv\n",
    "#WL = pd.DataFrame.from_csv(maindir+'Water_Level_data/'+site_name+'raw_data_ZentraAPI.csv') # Local file\n",
    "WL = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Water_Level_data/'+site_name+'_raw_data_ZentraAPI.csv') # GitHub\n",
    "## Inspect the data from Zentra to make sure it looks right...\n",
    "WL.ix[start_time_loc:end_time_loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Rain data for deliverable\n",
    "## Hourly Rain data for analysis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain gauge used for SDR-041 is Los Coches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rain_in</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-02</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-03</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-04</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-06</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-07</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-08</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-09</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-10</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-11</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-12</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-13</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-14</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-15</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-16</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-17</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-18</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-19</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-20</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-21</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-22</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-23</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-24</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-25</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-26</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-28</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-29</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-30</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-02</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-03</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rain_in\n",
       "2020-05-01     0.00\n",
       "2020-05-02     0.00\n",
       "2020-05-03     0.00\n",
       "2020-05-04     0.00\n",
       "2020-05-05     0.00\n",
       "2020-05-06     0.00\n",
       "2020-05-07     0.00\n",
       "2020-05-08     0.00\n",
       "2020-05-09     0.01\n",
       "2020-05-10     0.00\n",
       "2020-05-11     0.00\n",
       "2020-05-12     0.00\n",
       "2020-05-13     0.01\n",
       "2020-05-14     0.00\n",
       "2020-05-15     0.00\n",
       "2020-05-16     0.00\n",
       "2020-05-17     0.00\n",
       "2020-05-18     0.00\n",
       "2020-05-19     0.00\n",
       "2020-05-20     0.00\n",
       "2020-05-21     0.00\n",
       "2020-05-22     0.00\n",
       "2020-05-23     0.00\n",
       "2020-05-24     0.00\n",
       "2020-05-25     0.00\n",
       "2020-05-26     0.00\n",
       "2020-05-27     0.00\n",
       "2020-05-28     0.00\n",
       "2020-05-29     0.00\n",
       "2020-05-30     0.00\n",
       "2020-05-31     0.00\n",
       "2020-06-01     0.00\n",
       "2020-06-02     0.00\n",
       "2020-06-03     0.00"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the rain gauge used for the site\n",
    "#rain_gauge_site_list = pd.DataFrame.from_csv(maindir+'Ancillary_files/Rain_gauge_to_sites_list.csv') # Local File\n",
    "rain_gauge_site_list = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Rain_gauge_to_sites_list.csv',index_col=0) # GitHub\n",
    "rain_gauge_name = rain_gauge_site_list.ix[site_name]['rain_gauge_name']\n",
    "print 'Rain gauge used for '+site_name+' is '+ rain_gauge_name\n",
    "## Rain gauges indexed by rain gauge name\n",
    "#Rain_gauge_info = pd.DataFrame.from_csv(maindir+'Ancillary_files/Rain_gauge_info.csv') # Local File\n",
    "Rain_gauge_info = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Rain_gauge_info.csv',index_col=0) # GitHub\n",
    "start_date, end_date = start_time_loc.strftime('%Y-%m-%d'), end_time_loc.strftime('%Y-%m-%d')\n",
    "#Rain1D = pd.DataFrame.from_csv(maindir+'Rain_data/'+rain_gauge_name+'_daily_'+start_date+'-'+end_date+'.csv') # Local file\n",
    "\n",
    "Rain1D = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Rain_data/'+rain_gauge_name.replace(' ','%20')+'_daily.csv') # GitHub\n",
    "Rain1H = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Rain_data/'+rain_gauge_name.replace(' ','%20')+'_hourly.csv') # GitHub\n",
    "## Check output \n",
    "Rain1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offsetting/Calibrating Water Level Data\n",
    "***\n",
    "1. Apply special offsets and clip bad data (storms will be clipped later)\n",
    "2. Calculate and apply offset from calibration points \n",
    "3. global offset to manually adjust\n",
    "#### >>> FINAL OFFSET saved to Google Sheets\n",
    "####  \n",
    "### 1a. Special Offsets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Add column of zero for data offset\n",
    "WL['spec_offset'] = 0.\n",
    "## Special Offsets and Bad Data Clips, Global Offset from Google Sheets\n",
    "spec_offsets  = open_2020_ClipsOffsets()[0]\n",
    "## Get offsets for each site\n",
    "offsets_list_for_site = spec_offsets[spec_offsets.index  == site_name]\n",
    "offsets_list_for_site\n",
    "## THIS IS AS TUPLES SO THE TUPLE IS INDEXED BY NUMBER NOT STRING\n",
    "for spec_offset in offsets_list_for_site.itertuples():\n",
    "    print ('Special offsets from Google sheet: ')\n",
    "    #print offset\n",
    "    ## set data in bad_data indices to nan\n",
    "    if pd.notnull(spec_offset.Start)==True and pd.notnull(spec_offset.End)==True:\n",
    "        print ('Special offset: '+spec_offset.Start.strftime('%m/%d/%y %H:%M')+' - '+spec_offset.End.strftime('%m/%d/%y %H:%M')+' = '+str(spec_offset.SpecialOffset_in)+ ' inches')\n",
    "        ## insert each offset value to Offset column\n",
    "        WL.loc[spec_offset.Start:spec_offset.End, ['spec_offset']] = spec_offset.SpecialOffset_in\n",
    "    else:\n",
    "        pass\n",
    "    print ('')   \n",
    "## Apply all offsets for unique shifts due to bad data or other issues\n",
    "WL['Level_spec_off'] = WL['in Water Level'] + WL['spec_offset']\n",
    "WL['Level_spec_off'] = WL['Level_spec_off'].round(2)\n",
    "WL[['in Water Level','spec_offset','Level_spec_off']].head()\n",
    "#WL[['in Water Level','Level_spec_off']].plot(figsize=(14,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Clip bad data (don't want it used with Field Measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips =  open_2020_ClipsOffsets()[2]\n",
    "\n",
    "try: # one entry dataframes are weird\n",
    "    clips_for_site = pd.DataFrame(clips.loc[site_name,:])\n",
    "    bad_data_clips = clips_for_site[clips_for_site['Reason'].isin(['Invalid','Obstruction'])]\n",
    "except:\n",
    "    try:\n",
    "        clips_for_site = pd.DataFrame(clips.loc[site_name,:]).T # have to make DF and Transpose it \n",
    "        bad_data_clips = clips_for_site[clips_for_site['Reason'].isin(['Invalid','Obstruction'])]\n",
    "    except KeyError:\n",
    "        print 'No clips found'\n",
    "        bad_data_clips = pd.DataFrame()\n",
    "    \n",
    "## iterate over list of bad data and clip from 'offset_flow_clipped'....\n",
    "print ('Clipping bad/invalid data....')\n",
    "WL['Level_spec_off_clipbad'] = WL['Level_spec_off']\n",
    "for clip in bad_data_clips.iterrows():\n",
    "    clip_start, clip_end = clip[1]['Start'], clip[1]['End']\n",
    "    if pd.isnull(clip_start)==False and pd.isnull(clip_end) == False:\n",
    "        print ('Clipped Invalid data from: '+clip_start.strftime('%m/%d/%y %H:%M')+'-'+clip_end.strftime('%m/%d/%y %H:%M'))\n",
    "        ## set data in WL indices to nan\n",
    "        WL.loc[clip_start:clip_end, ['Level_spec_off_clipbad']] = np.nan\n",
    "    else:\n",
    "        print ('No data to clip...')\n",
    "        pass   \n",
    "#WL[['in Water Level','Level_spec_off','Level_spec_off_clipbad']].plot(figsize=(16,4))\n",
    "bad_data_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate offset from calibration measurements\n",
    "### Field Data for Calibrations\n",
    "### Load from Google Sheets or csv if already saved\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Open FDS from Google Sheets\n",
    "fds = open_fds_from_google_sheet()\n",
    "fds['NOTES'] = fds['NOTES'].replace(np.nan, '', regex=True)\n",
    "## Save to csv\n",
    "fds.to_csv(maindir+'Ancillary_files/FDS/Field_data_sheet_backup.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load from csv\n",
    "fds = pd.DataFrame.from_csv(maindir+'Ancillary_files/FDS/Field_data_sheet_backup.csv')\n",
    "## Round to 5Min\n",
    "fds['Datetime'] = pd.to_datetime(fds['Date and Time']).apply(lambda x: dt.datetime(x.year, x.month, x.day, x.hour,5*(x.minute // 5)))\n",
    "## Make Index line up with Excel row numbers for easy reference\n",
    "fds.index+=2\n",
    "## Drop any duplicate rows so it doesn't weight the average \n",
    "fds_len =  len(fds)\n",
    "fds = fds.drop_duplicates(keep='first')\n",
    "fds_len_no_dup = len(fds)\n",
    "diff = fds_len - fds_len_no_dup\n",
    "print ('')\n",
    "print ('Dropped '+str(diff)+' duplicate rows')\n",
    "print ('')\n",
    "## strip commas from volume measurement\n",
    "fds['Flow Measurement, Volume in mL'] = fds['Flow Measurement, Volume in mL'].apply(lambda x: x.replace(',',''))\n",
    "## cm to inches\n",
    "fds['Level_above_V_cm'] = fds['Height above (+) or below (-) v-notch in cm']\n",
    "fds['Level_above_V_in'] = np.round(fds['Level_above_V_cm'] / 2.54, 2)\n",
    "## Flow in cfs: mL to cfs divided by seconds\n",
    "fds['Flow_meas_gpm'] = (fds['Flow Measurement, Volume in mL'].astype('float') / fds['Flow Measurement, Time in Seconds '].astype('float')) * 0.02 #1mL per second is 0.02 gpm\n",
    "fds['Flow_meas_gpm'] = fds['Flow_meas_gpm'].round(3)\n",
    "## Display measurements\n",
    "fds[fds['Site ID']==site_name][['Site ID','Datetime','Level_above_V_in','Flow_meas_gpm','NOTES']]\n",
    "#fds[['Level_above_V_in','Flow_meas_gpm']].plot.scatter('Level_above_V_in','Flow_meas_gpm',figsize=(8,6))\n",
    "#fds[['Site ID','Datetime','Date and Time','Flow Condition',u'NOTES',u'Site Photo or Video?', u'Height above (+) or below (-) v-notch in cm',u'Flow Measurement, Time in Seconds ',u'Flow Measurement, Volume in mL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Offset from Calibration Points for Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Choose time for selecting points\n",
    "cal_start, cal_end = start_time_loc, end_time_loc\n",
    "# FIELD MEASUREMENTS\n",
    "field_meas = fds[fds['Site ID']==site_name][['Datetime','Level_above_V_in','Flow_meas_gpm']]\n",
    "## Add PT level data to field measured level \n",
    "for t in field_meas['Datetime'].values:\n",
    "    t = pd.to_datetime(t)\n",
    "    #print ('Field measurement time:' + str(t))\n",
    "    try:\n",
    "        #print 'Level data from Meter: '\n",
    "        #print WL.loc[t]['Level_spec_off_clipbad'] ## water level after manual offset\n",
    "        field_meas.loc[field_meas['Datetime']==t, 'Level_spec_off_clipbad'] = WL.loc[t]['Level_spec_off_clipbad']\n",
    "    except:\n",
    "        try:\n",
    "            ' Shifting calibration time back 5 miutes....'\n",
    "            t = t - dt.timedelta(minutes=5)\n",
    "            field_meas.loc[field_meas['Datetime']==t, 'Level_in'] = WL.loc[t]['Level_in']\n",
    "        except:\n",
    "            pass\n",
    "## Add the flow that would be predicted from v-notch equation\n",
    "try: \n",
    "    field_meas.loc[:,'Predicted_flow'] =  [HvF.loc[np.round(x,2)]['Q (GPM)'] for x in field_meas['Level_above_V_in'].values]\n",
    "except KeyError:\n",
    "    #field_meas_level_nozeros = field_meas_level[field_meas_level['Level_above_V_in'] >=0.]\n",
    "    #field_meas_level_nozeros.loc['Predicted_flow'] = 0.\n",
    "    field_meas.loc[:,'Predicted_flow'] = 0.\n",
    "## Display field measurements of level and flow \n",
    "display(field_meas)\n",
    "## Calibration measurements made in May, QC measurements made June-Sept\n",
    "field_meas_QC = field_meas[field_meas['Datetime']>cal_end] ## after 5/31 should be only QC measurements\n",
    "field_meas_Cal = field_meas[(field_meas['Datetime']>=cal_start) & (field_meas['Datetime']<=cal_end)]\n",
    "\n",
    "## Calculate average offset from field data\n",
    "field_meas_Cal['calculated offset'] = field_meas_Cal['Level_above_V_in'] - field_meas_Cal['Level_spec_off_clipbad'] ## Measured - Manually adjusted PT reading\n",
    "## Calculate total offset  \n",
    "calculated_offset = field_meas_Cal['calculated offset'].mean()\n",
    "#print 'Calculated offset = '+str(calculated_offset)+' in'\n",
    "## Copy over data that has already had special offsets applied and bad data clipped\n",
    "## Apply calculated offset\n",
    "WL['Level_spec_off_clipbad_calc_off'] = WL['Level_spec_off_clipbad'] + calculated_offset\n",
    "## Display water level data\n",
    "fig, ax1 = plt.subplots(1,1,figsize=(14,4))\n",
    "ax1.plot(WL.index,WL['in Water Level'],marker='None',ls='-',c='grey',label='Raw Level data')\n",
    "ax1.plot(WL.index,WL['Level_spec_off'],marker='None',ls='-',c='red',label='Bad data/Obstructed clipped out')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad'],marker='None',ls='-',c='orange',label='Raw Level data + special offset')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad_calc_off'],marker='None',ls='-',c='green',label='Level offset by calibrations')\n",
    "## Plot field measurements\n",
    "from matplotlib.dates import DateFormatter, date2num\n",
    "ax1.plot_date(date2num(field_meas_Cal['Datetime'].values),field_meas_Cal['Level_above_V_in'].values,marker='s',c='b',label='Initial Calibration measurements')\n",
    "#ax1.plot(date2num(field_meas_QC['Datetime'].values),field_meas_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "ax1.axhline(0,c='k')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey',ls='--')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k',ls='--')\n",
    "ax1.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Global Offset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get global offset (stored in Google Sheets)\n",
    "glob_offsets = open_2020_ClipsOffsets()[1]\n",
    "glob_offset = glob_offsets.ix[site_name]['GlobalOffset_in']\n",
    "print 'Global offset for '+site_name+' = '+str(glob_offset) +' in.'\n",
    "## Apply global offset\n",
    "WL['Level_spec_off_clipbad_calc_off_glob_off'] = WL['Level_spec_off_clipbad_calc_off'] + glob_offset\n",
    "## Final, offset and cleaned water level data for flow calculation\n",
    "WL['Level_in'] = WL['Level_spec_off_clipbad_calc_off_glob_off']\n",
    "## Highlight missing data  \n",
    "missing_data = pd.DataFrame(WL[np.isnan(WL['Level_in'])]['Level_in'])\n",
    "missing_data['missing_level_data'] = 0.\n",
    "missing_data = missing_data.reindex(index=pd.date_range(start_time_loc,end_time_loc,freq='5Min'))\n",
    "## Display water level data\n",
    "fig, ax1 = plt.subplots(1,1,figsize=(14,4))\n",
    "#ax1.plot(WL.index,WL['in Water Level'],marker='None',ls='-',c='grey',label='Raw Level data')\n",
    "#ax1.plot(WL.index,WL['Level_spec_off'],marker='None',ls='-',c='red',label='Bad data clipped out')\n",
    "#ax1.plot(WL.index,WL['Level_spec_off_clipbad'],marker='None',ls='-',c='orange',label='Raw Level data + special offset')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad_calc_off'],marker='None',ls='-',c='green',label='Level offset by calibrations')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad_calc_off_glob_off'],marker='None',ls='-',c='blue',label='Final Level data, global offset:'+str(glob_offset) +' in.')\n",
    "## Plot field measurements\n",
    "from matplotlib.dates import DateFormatter, date2num\n",
    "ax1.plot_date(date2num(field_meas_Cal['Datetime'].values),field_meas_Cal['Level_above_V_in'].values,marker='s',c='k',label='Initial Calibration measurements')\n",
    "#ax1.plot(date2num(field_meas_QC['Datetime'].values),field_meas_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "ax1.axhline(0,c='k')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey',ls='--')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k',ls='--')\n",
    "ax1.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Total offset\n",
    "#### >>> Saved in Google Sheets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOffsets = open_2020_FinalOffsets()\n",
    "FinalOffsets = FinalOffsets[['CalculatedOffset_in','GlobalOffset_in','FinalOffset_in']]\n",
    "FinalOffsets.loc[site_name,'CalculatedOffset_in'] = np.round(calculated_offset,2)\n",
    "FinalOffsets.loc[site_name,'GlobalOffset_in'] = np.round(glob_offset,2)\n",
    "FinalOffset = calculated_offset + glob_offset\n",
    "FinalOffsets.loc[site_name,'FinalOffset_in'] = np.round(FinalOffset,2)\n",
    "save_df_to_GoogleSheets(FinalOffsets, worksheet_name='FinalOffsets',spreadsheet_key=\"1U0UnBJrpMNEtDYctO2GW0fuobdc8vJfdLIWbvSr--ss\")\n",
    "FinalOffsets.to_csv(maindir+'Ancillary_files/FinalOffsets_backup.csv')\n",
    "## Display\n",
    "FinalOffsets.ix[site_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Flow\n",
    "### HvF table for 90deg v-notch until water level exceeds v, then CTRSC equation\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## HvF table for 90 deg v-notch\n",
    "def level_to_gpm_vnotch(x):\n",
    "    if x < 0:\n",
    "        gpm = 0.0\n",
    "    else:\n",
    "        gpm = HvF.loc[np.round(x,2)]['Q (GPM)']\n",
    "    return gpm\n",
    "WL['Flow_gpm_v'] = WL['Level_in'].apply(lambda x: level_to_gpm_vnotch(x))    \n",
    "## Calculate flows when overtopping the weir\n",
    "WL['Flow_gpm'] = CTRSC_compound_weir(site_name, WL, weir_dims)#,  True, True)\n",
    "#WL[['Level_in','Flow_gpm']].plot(figsize=(14,4),grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC Hydrograph and Clip storm flow\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Clip storm flows\n",
    "clips =  open_2020_ClipsOffsets()[2] # reload every time so Google sheet can be updated\n",
    "try: # one entry dataframes are weird\n",
    "    clips_for_site = pd.DataFrame(clips.loc[site_name,:])\n",
    "    storm_clips = clips_for_site[clips_for_site['Reason']=='Storm']\n",
    "except:\n",
    "    try:\n",
    "        clips_for_site = pd.DataFrame(clips.loc[site_name,:]).T # have to make DF and Transpose it \n",
    "        storm_clips = clips_for_site[clips_for_site['Reason']=='Storm'] \n",
    "    except KeyError:\n",
    "        storm_clips = pd.DataFrame()\n",
    "## iterate over list of bad data and clip from 'Flow_gpm'....\n",
    "print ('Clipping stormflow data....')\n",
    "WL['Flow_gpm_storm_clipped'] = WL['Flow_gpm']\n",
    "for clip in storm_clips.iterrows():\n",
    "    clip_start, clip_end = clip[1]['Start'], clip[1]['End']\n",
    "    if pd.isnull(clip_start)==False and pd.isnull(clip_end) == False:\n",
    "        print ('Clipped storm data from: '+clip_start.strftime('%m/%d/%y %H:%M')+'-'+clip_end.strftime('%m/%d/%y %H:%M'))\n",
    "        ## set data in WL indices to nan\n",
    "        WL.loc[clip_start:clip_end, ['Flow_gpm_storm_clipped']] = np.nan\n",
    "    else:\n",
    "        print ('No data to clip...')\n",
    "        pass   \n",
    "storm_clips\n",
    "\n",
    "########################################\n",
    "##### QC Hydrograph ###################\n",
    "#####################################\n",
    "fig, (ax1, ax2, ax4) = plt.subplots(3,1,figsize=(14,7),sharex=True)\n",
    "## Plot full scale level data\n",
    "ax1.plot_date(missing_data.index,missing_data['missing_level_data'],marker='None',ls='-',c='r',label='Missing/Obstructed Data')\n",
    "## raw\n",
    "ax1.plot_date(WL.index, WL['in Water Level'], marker='None',ls='-',c='grey',alpha=0.25,label='Raw level data')\n",
    "##raw +final offset\n",
    "ax1.plot_date(WL.index, WL['Level_in'], marker='None',ls='-',c='g',alpha=0.25,label='Raw level + FinalOffset ('+\"%.2f\"%FinalOffset+' in.)')\n",
    "## Plot field measurements\n",
    "ax1.plot_date(field_meas_Cal['Datetime'].values,field_meas_Cal['Level_above_V_in'].values,marker='s',c='b',label='Initial Calibration measurements')\n",
    "ax1.plot_date(field_meas_QC['Datetime'].values,field_meas_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k')\n",
    "textstr = 'Weir crest height: '+str(weir_dims.loc[site_name,'h2'])+' inches'\n",
    "#ax1.annotate(textstr, (mpl.dates.date2num(pd.to_datetime(weir_dims.loc['CAR-007','Measurement date'])),weir_dims.loc[site_name,'h2']))\n",
    "## Plot temp\n",
    "#ax1_1 = ax1.twinx()\n",
    "#ax1_1.plot_date(WL.index, WL['Temp_F'], marker='None',ls='-',c='grey',label='Temp F')\n",
    "\n",
    "## Plot full scale flow data or\n",
    "## Conductivity data if available\n",
    "if u'mS/cm EC' in WL.columns:\n",
    "    ax2.plot_date(WL.index, WL[u'mS/cm EC'], marker='None',ls='-',c='orange',label= 'mS/cm EC')\n",
    "    ax2.set_ylabel('Sp.Cond (mS/cm)',color='orange'), \n",
    "else:\n",
    "    ax2.plot_date(WL.index, WL['Flow_gpm'], marker='None',ls='-',c='teal',label='Flow (gpm)')\n",
    "    ax2.set_ylabel('Flow (gpm)',color='b'), \n",
    "## Put notes on the plot\n",
    "for row in fds[fds['Site ID']==site_name][['Datetime','NOTES']].iterrows():\n",
    "    note = '\\n'.join(textwrap.wrap(str(row[1]['NOTES']), 16))\n",
    "    ax2.annotate(note,xy=(pd.to_datetime(row[1]['Datetime']),WL['Flow_gpm'].mean()),rotation=90,verticalalignment='bottom')\n",
    "    ax2.axvline(pd.to_datetime(row[1]['Datetime']),color='grey',alpha=0.5)\n",
    "### Plot precip on inverted, secondary y axis\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot_date(Rain1H.index, Rain1H['Rain_in'], marker='None',ls='steps-mid',color='teal',label='Precip: '+rain_gauge_name)\n",
    "## Plot flow data, compound weir\n",
    "ax4.plot_date(WL.index, WL['Flow_gpm'], marker='None',ls='-',c='grey',alpha=0.5,label='Flow (gpm)')\n",
    "## Plot flow data, storms clipped\n",
    "ax4.plot_date(WL.index, WL['Flow_gpm_storm_clipped'], marker='None',ls='-',c='green',label='Flow(gpm), storms clipped')\n",
    "## Plot calibration field measurements\n",
    "ax4.plot_date(field_meas_Cal['Datetime'].values,field_meas_Cal['Flow_meas_gpm'].values,marker='o',c='b',label='Initial Calibration measurements')\n",
    "## Plot QC field measurements\n",
    "ax4.plot_date(field_meas_QC['Datetime'].values,field_meas_QC['Flow_meas_gpm'].values,marker='o',c='r',label='Follow-up QC measurements')\n",
    "\n",
    "## Previous deliverable data\n",
    "#ax4.plot_date(del_df.index,del_df['Flow compound weir (gpm)'], marker='None',ls='-',c='b',label='Previous deliverable')\n",
    "\n",
    "### Plot precip on inverted, secondary y axis\n",
    "ax4_2 = ax4.twinx()\n",
    "ax4_2.plot_date(Rain1H.index, Rain1H['Rain_in'], marker='None',ls='steps-mid',color='teal',label='Precip: '+rain_gauge_name)\n",
    "\n",
    "## Format/set limits\n",
    "## full scale flow\n",
    "ax1.set_ylim(-3, WL['Level_in'].max() * 1.1)\n",
    "#ax2.set_ylim(-WL['offset_flow'].max() * 0.5, WL['offset_flow'].max() * 2.)\n",
    "ax3.set_ylim(0, Rain1H['Rain_in'].max() * 2.)\n",
    "ax4_2.set_ylim(0, Rain1H['Rain_in'].max() * 3.)\n",
    "ax3.invert_yaxis(), ax4_2.invert_yaxis()\n",
    "\n",
    "## low flow\n",
    "ax4.set_ylim(-WL['Flow_gpm_storm_clipped'].max() * 0.45, WL['Flow_gpm_storm_clipped'].max() * 1.2)\n",
    "## set x-axis to monitoring period\n",
    "ax1.set_xlim(start_time_loc, end_time_loc)\n",
    "ax1.grid(True)\n",
    "ax1.set_ylabel('Level (inches)',color='g')\n",
    "ax3.set_ylabel('Precip (inches)',color='teal')\n",
    "ax4.set_ylabel('Flow (gpm)',color='b')\n",
    "\n",
    "ax1.legend(fontsize=6,numpoints=1,ncol=2,loc='upper left')\n",
    "ax2.legend(fontsize=6,loc='lower left'), ax3.legend(fontsize=12,loc='lower right')\n",
    "ax4.legend(fontsize=6,numpoints=1,ncol=5,loc='lower left')\n",
    "\n",
    "ax4.xaxis.set_major_formatter(mpl.dates.DateFormatter('%A \\n %m/%d/%y %H:%M'))\n",
    "\n",
    "fig.suptitle('Data processing for site: '+site_name,fontsize=12,fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds[fds['Site ID']==site_name][['Site ID','Datetime','Level_above_V_in','Flow_meas_gpm','NOTES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QC SCATTERPLOTS\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(16,5))\n",
    "fig.suptitle(site_name,fontweight='bold',fontsize=8)\n",
    "\n",
    "## MEASURED LEVEL VS OFFSET LEVEL\n",
    "field_meas_Cal['Final Level Data (in)'] =  field_meas_Cal['Datetime'].apply(lambda x: WL.loc[x]['Level_in'])\n",
    "field_meas_Cal['Level_diff_in'] = field_meas_Cal['Level_above_V_in'] - field_meas_Cal['Final Level Data (in)'] \n",
    "if len(field_meas_QC)>0:\n",
    "    field_meas_QC['Final Level Data (in)'] =  field_meas_QC['Datetime'].apply(lambda x: WL.loc[x]['Level_in'])\n",
    "    field_meas_QC['Level_diff_in'] = field_meas_QC['Level_above_V_in'] - field_meas_QC['Final Level Data (in)'] \n",
    "one_to_one = ax1.plot([-10,1000],[-10,1000],ls='-',marker='None',color='grey',alpha=0.5,label='1:1')\n",
    "one_to_one_plus = ax1.plot([-10,1000],[-10.1,1000.1],ls='--',marker='None',color='grey',alpha=0.5,label='+0.1')\n",
    "one_to_one = ax1.plot([-10,1000],[-9.9,999.9],ls='--',marker='None',color='grey',alpha=0.5,label='-0.1')\n",
    "Calpoints = ax1.plot(field_meas_Cal['Level_above_V_in'],field_meas_Cal['Final Level Data (in)'],ls='None',marker='o',c='b',markersize=12,label='Initial Calibration measurements')\n",
    "#QCpoints = ax1.plot(field_meas_QC['Level_above_V_in'],field_meas_QC['Final Level Data (in)'],ls='None',marker='o',c='r',markersize=12,label='Follow-up QC measurements')   \n",
    "ax1.set_xlabel('Measured Level (in)',fontweight='bold',fontsize=10)\n",
    "ax1.set_ylabel('Final Level data (in)',fontweight='bold',fontsize=10)\n",
    "max_level = field_meas_Cal[[u'Level_above_V_in', u'Final Level Data (in)']].max().max()\n",
    "ax1.set_xlim(0, 1.3 *max_level.max())\n",
    "ax1.set_ylim(0, 1.3 *max_level.max())\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_title('Measured level vs Offset Level',fontweight='bold',fontsize=12)\n",
    "ax1.grid(True)\n",
    "hover_points(Calpoints, list(field_meas_Cal['Datetime']), fig, ax1)\n",
    "#hover_points(QCpoints, list(field_meas_QC['Datetime']), fig, ax1)\n",
    "\n",
    "## PREDICTED FLOW VS MEASURED FLOW\n",
    "one_to_one = ax2.plot([0,1000],[0,1000],ls='-',marker='None',color='grey',alpha=0.5,label='1:1')\n",
    "Calpoints = ax2.plot(field_meas_Cal['Flow_meas_gpm'],field_meas_Cal['Predicted_flow'],ls='None',marker='o',markersize=12,label='Initial Calibration measurements',c='b')\n",
    "QCpoints = ax2.plot(field_meas_QC['Flow_meas_gpm'],field_meas_QC['Predicted_flow'],ls='None',marker='o',markersize=12,label='Follow-up QC measurements',c='r')\n",
    "ax2.set_xlabel('Measured Flow (gpm)',fontweight='bold',fontsize=10)\n",
    "ax2.set_ylabel('Predicted flow from Measured Level (gpm)',fontweight='bold',fontsize=10)\n",
    "ax2.set_xlim(0, 1.3 *field_meas_Cal['Predicted_flow'].max())\n",
    "ax2.set_ylim(0, 1.3 *field_meas_Cal['Predicted_flow'].max())\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_title('Measured flow vs Predicted flow from measured level in v-notch',fontweight='bold',fontsize=10)\n",
    "hover_points(Calpoints, list(field_meas_Cal['Datetime']), fig, ax2)\n",
    "hover_points(QCpoints, list(field_meas_QC['Datetime']), fig, ax2)\n",
    "ax2.grid(True)\n",
    "\n",
    "## MEASURED FLOW VS FINAL FLOW\n",
    "field_meas_Cal['Final Flow Data (gpm)'] =  field_meas_Cal['Datetime'].apply(lambda x: WL.loc[x]['Flow_gpm'])\n",
    "field_meas_QC['Final Flow Data (gpm)'] =  field_meas_QC['Datetime'].apply(lambda x: WL.loc[x]['Flow_gpm'])\n",
    "\n",
    "## Calculate flow differences in gpm and %\n",
    "field_meas_Cal['difference_gpm'] = field_meas_Cal['Final Flow Data (gpm)'] - field_meas_Cal['Flow_meas_gpm']\n",
    "field_meas_Cal['difference_%'] = (abs(field_meas_Cal['Final Flow Data (gpm)'] - field_meas_Cal['Flow_meas_gpm']) / field_meas_Cal['Flow_meas_gpm']) * 100.\n",
    "print 'Average difference between measured, and final processed flow values: '+\"%.3f\"%field_meas_Cal['difference_gpm'].mean() +' gpm'\n",
    "print 'Average difference between measured, and final processed flow values: '+\"%.1f\"%field_meas_Cal['difference_%'].mean() +' %'\n",
    "one_to_one = ax3.plot([0,1000],[0,1000],ls='-',marker='None',color='grey',alpha=0.5,label='1:1')\n",
    "Calpoints = ax3.plot(field_meas_Cal['Flow_meas_gpm'],field_meas_Cal['Final Flow Data (gpm)'],ls='None',marker='o',markersize=12,label='Initial Calibration measurements',c='b')\n",
    "QCpoints = ax3.plot(field_meas_QC['Flow_meas_gpm'],field_meas_QC['Final Flow Data (gpm)'],ls='None',marker='o',markersize=12,label='Follow-up QC measurements',c='r')\n",
    "ax3.set_xlabel('Measured Flow (gpm)',fontweight='bold',fontsize=10)\n",
    "ax3.set_ylabel('Flow data Output (gpm)',fontweight='bold',fontsize=10)\n",
    "ax3.set_xlim(0, 1.3 *field_meas_Cal[['Final Flow Data (gpm)','Flow_meas_gpm']].max().max())\n",
    "ax3.set_ylim(0, 1.3 *field_meas_Cal[['Final Flow Data (gpm)','Flow_meas_gpm']].max().max())\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.set_title('Measured flow vs Flow data output',fontweight='bold',fontsize=12)\n",
    "hover_points(Calpoints, list(field_meas_Cal['Datetime']),fig, ax3)\n",
    "hover_points(QCpoints, list(field_meas_QC['Datetime']),fig, ax3)\n",
    "ax3.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)\n",
    "\n",
    "display(field_meas_Cal[[u'Datetime', u'Level_above_V_in', u'Final Level Data (in)','Level_diff_in']])\n",
    "display(field_meas_Cal[['Datetime','Flow_meas_gpm','Final Flow Data (gpm)','difference_gpm','difference_%']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrograph Separation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.990\n",
    "\n",
    "flow_df = WL[[u'Flow_gpm']]\n",
    "## gap fill\n",
    "flow_df = flow_df.fillna(flow_df.interpolate(method='linear')).fillna(flow_df.mode().loc[0].values[0])\n",
    "#flow_df[''Flow_compound_weir'].plot(c='b')\n",
    "## Function to add original flow peaks back into dataset\n",
    "def peaks(original_flow, smoothed, peak_val=2):\n",
    "    if abs(original_flow - smoothed) > peak_val:\n",
    "        flow = original_flow\n",
    "    else:\n",
    "        flow = smoothed\n",
    "    return flow\n",
    "## Smoothing\n",
    "flow_df['rolling'] = flow_df['Flow_gpm'].rolling(12,min_periods=3,center=True).mean()\n",
    "#flow_df['rolling'].plot(c='g') \n",
    "## Add peaks back into rolling data\n",
    "flow_df['rolling+peaks']  = flow_df.apply(lambda x: peaks(x['Flow_gpm'],x['rolling']), axis=1)\n",
    "#flow_df['rolling+peaks'].plot(c='r')\n",
    "## Butter filter\n",
    "b, a = signal.butter(3, 0.2, btype='lowpass', analog=False) ## 0.2 parameter selected by trial and error\n",
    "flow_df['butter'] = signal.filtfilt(b, a, flow_df['rolling+peaks'])\n",
    "#flow_df['butter'].plot(c='orange')\n",
    "flow_df['butter+peaks']  = flow_df.apply(lambda x: peaks(x['Flow_gpm'],x['butter'], 1.), axis=1)\n",
    "## CHoose a smoothed dataset to apply the DF to\n",
    "flow_df['Flow (gpm) smooth'] = flow_df['butter+peaks']\n",
    "## Set arbitrary index\n",
    "flow_df = flow_df.reset_index()\n",
    "## Baseflow df    \n",
    "df = flow_df\n",
    "## Define h_k1 (original flow data series\n",
    "df['h_k1'] = df['Flow (gpm) smooth']\n",
    "### BACKWARD FILTER\n",
    "## Fill in first value for q_k-1\n",
    "df.loc[0,'q_k-1'] = df.loc[0,'h_k1']\n",
    "## q_k-1\n",
    "for i in range(1,len(df)):\n",
    "    # (0.925 * q_k-1) + (((1+0.925)/2) * (q_k - q_k-1))\n",
    "    df.loc[i,'q_k-1'] = (alpha*df.loc[i-1,'q_k-1']) + (((1.+alpha)/2.) * (df.loc[i,'h_k1'] - df.loc[i-1,'h_k1']))\n",
    "## Change negatives to 0's\n",
    "df['q_k-1>0'] = df['q_k-1'].where(df['q_k-1']>0., 0.)\n",
    "## b_k1\n",
    "df['b_k1'] = df['h_k1'] - df['q_k-1>0']\n",
    "## FORWARD FILTER\n",
    "df['h_k2'] = df['h_k1'] - df['b_k1']\n",
    "## Fill in first value for q_k+1\n",
    "df.loc[df.index[-1],'q_k+1'] = 0.\n",
    "##q_k+1\n",
    "for i in range(df.index[-2],0,-1): ## iterate backwards\n",
    "    # (0.925 * q_k+1) + (((1+0.925)/2) * (q_k - q_k-1))\n",
    "    df.loc[i,'q_k+1'] = (alpha*df.loc[i+1,'q_k+1']) + (((1.+alpha)/2.) * (df.loc[i,'b_k1'] - df.loc[i+1,'b_k1']))\n",
    "## change negatives to 0's\n",
    "df['q_k+1>0'] = df['q_k+1'].where(df['q_k+1']>=0, 0.)\n",
    "## b_k2\n",
    "df['b_k2'] = df['b_k1'] - df['q_k+1>0']\n",
    "\n",
    "## Rest values back to date\n",
    "df = df.set_index(df['index'])\n",
    "## Deliver\n",
    "df[['h_k1','b_k1','b_k2']]\n",
    "\n",
    "flowoutput = df[['Flow_gpm','Flow (gpm) smooth']]\n",
    "flowoutput.loc[:,'Baseflow (gpm)'] = df['b_k2']\n",
    "flowoutput.loc[:,'Quickflow (gpm)'] = df['Flow (gpm) smooth'] - df['b_k2']\n",
    "\n",
    "## Put in original flow data and Mask where Nan values in orginal dataset\n",
    "flowoutput.loc[:,'Flow_gpm'] = WL[[u'Flow_gpm']]\n",
    "m = pd.notnull(flowoutput['Flow_gpm'])\n",
    "flowoutput = flowoutput.where(m, np.nan)   \n",
    "\n",
    "WL.loc[:,'Baseflow (gpm)'] = flowoutput['Baseflow (gpm)'].round(3)\n",
    "WL.loc[:,'Quickflow (gpm)'] = flowoutput['Quickflow (gpm)'].round(3)\n",
    "\n",
    "## Drop data for data dropouts\n",
    "WL['Baseflow (gpm)'] = np.where(WL['Level_in'].isnull(),np.nan,WL['Baseflow (gpm)'])\n",
    "WL['Baseflow (gpm)'] = np.where(WL['Level_in']<0.,0.,WL['Baseflow (gpm)']) ## Get rid of negative values\n",
    "WL['Quickflow (gpm)'] = np.where(WL['Level_in'].isnull(),np.nan,WL['Quickflow (gpm)'])\n",
    "\n",
    "#%% BASEFLOW PLOT\n",
    "## PLOT\n",
    "#    fig, (ax1,ax2,ax3) = plt.subplots(3,1,figsize=(20,10),sharex=True)\n",
    "fig, ax1 = plt.subplots(1,1,figsize=(14,6))\n",
    "fig.suptitle(site_name,fontsize=14, fontweight='bold')\n",
    "\n",
    "## Flow\n",
    "ax1.plot_date(WL.index, WL['Flow_gpm'],marker='None',ls='-',label='Orig. Flow Data (gpm)',c='grey')\n",
    "ax1.plot_date(flowoutput.index, flowoutput['Flow (gpm) smooth'],marker='None',ls='-',label='Smoothed Flow Data (gpm)',c='b')\n",
    "#ax1.plot_date(WL.index, WL['Baseflow (gpm)'] + WL['Quickflow (gpm)'],marker='None',ls='-',label='Quickflow (digital filter='+str(alpha)+')',c='r')\n",
    "ax1.plot_date(WL.index, WL['Baseflow (gpm)'],marker='None',ls='-',label='Baseflow (digital filter='+str(alpha)+')',c='g')\n",
    "ax1.set_ylabel('Flow (gpm)',fontweight='bold',fontsize=14)\n",
    "for ax in fig.axes:\n",
    "    ax.legend(loc='upper left')\n",
    "ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%m-%d %H:%M'))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95,hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE TO EXCEL\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def xl_columnrow(col,row=''):\n",
    "    \"\"\" Convert given row and column number to an Excel-style cell name. \"\"\"\n",
    "    LETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    result = []\n",
    "    while col:\n",
    "        col, rem = divmod(col-1, 26)\n",
    "        result[:0] = LETTERS[rem]\n",
    "    return ''.join(result)+str(row)\n",
    "\n",
    "## Get rid of all data for data dropouts\n",
    "for col in WL.columns:\n",
    "    #print col\n",
    "    try:\n",
    "        WL[col] = np.where(WL['Level_in'].isnull(),np.nan,WL[col])\n",
    "    except:\n",
    "        print 'skipped col: '+col\n",
    "        pass\n",
    "\n",
    "### FINALIZED FLOW OUTPUT\n",
    "## FLOW\n",
    "Corr_flow = WL[['Flow_gpm', 'Flow_gpm_storm_clipped']].round(3)\n",
    "Corr_flow.columns = ['Flow compound weir (gpm)', 'Flow compound weir stormflow clipped (gpm)']\n",
    "## Add base/quickflow\n",
    "Corr_flow[['Baseflow (gpm)','Quickflow (gpm)']] = WL[['Baseflow (gpm)','Quickflow (gpm)']]\n",
    "## Add temp and conductivity to deliverable\n",
    "Corr_flow[u'uS/cm EC'] = np.round(WL[u'mS/cm EC'] * 1000., 0)\n",
    "temp_col_name = [col for col in WL.columns if 'Water Temperature' in col]\n",
    "Corr_flow[u'\\xb0F Water Temperature'] = WL[temp_col_name].round(1)\n",
    "\n",
    "## PIVOT TABLE STUFF\n",
    "Corr_flow.loc[:,('Year')] = Corr_flow.index.year\n",
    "Corr_flow.loc[:,('Month')] = Corr_flow.index.month\n",
    "Corr_flow.loc[:,('Day')] = Corr_flow.index.day\n",
    "Corr_flow.loc[:,('Hour')] = Corr_flow.index.hour\n",
    "Corr_flow.loc[:,('Minute')] = Corr_flow.index.minute\n",
    "Corr_flow.loc[:,('Weekday')] = Corr_flow.index.map(lambda x: calendar.day_name[x.weekday()])\n",
    "\n",
    "## Kick out to Excel\n",
    "final_flow_ExcelFile = pd.ExcelWriter(maindir+'Flow_Output_Excel_files/'+site_name+'-working draft.xlsx')\n",
    "max_row, rain_max_row = Excel_Plots(site_name, Corr_flow, Rain1D, final_flow_ExcelFile, start_time_loc, end_time_loc)\n",
    "\n",
    "### Pivot TABLES\n",
    "## Old style-SUM but ADDING the multiplication by 5min (gpm->gp5M)\n",
    "PivotTable_Sum = pd.pivot_table(Corr_flow,values='Flow compound weir stormflow clipped (gpm)', columns=['Month','Day','Weekday'], index=['Hour'], aggfunc=np.sum).round(1) * 5. # *5 for 5Min interval data\n",
    "PivotTable_Sum.to_excel(final_flow_ExcelFile,site_name+'PivotTable-Sum')\n",
    "## Freeze Panes\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Sum'].freeze_panes(4, 1)\n",
    "## Conditional Formatting\n",
    "def rgb_hex(red,green,blue):\n",
    "    return '#%02x%02x%02x' % (red, green, blue)\n",
    "green, yellow, red = rgb_hex(99,190,123),rgb_hex(255,235,132),rgb_hex(248,105,107)\n",
    "max_col_row = xl_columnrow(len(PivotTable_Sum.columns)+1,28) #24th hour is on row 28\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Sum'].conditional_format('B5:'+max_col_row, {'type': '3_color_scale','min_color': green,'mid_color':yellow,'max_color':red})\n",
    "## Old style-AVG\n",
    "PivotTable_Avg = pd.pivot_table(Corr_flow,values='Flow compound weir stormflow clipped (gpm)', columns=['Month','Day','Weekday'], index=['Hour'], aggfunc=np.mean).round(3)\n",
    "PivotTable_Avg.to_excel(final_flow_ExcelFile,site_name+'PivotTable-Avg')\n",
    "## Freeze Panes\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Avg'].freeze_panes(4, 1)\n",
    "## Conditional Formatting\n",
    "max_col_row = xl_columnrow(len(PivotTable_Sum.columns)+1,28)  #24th hour is on row 28\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Avg'].conditional_format('B5:'+max_col_row, {'type': '3_color_scale','min_color': green,'mid_color': yellow,'max_color': red})\n",
    "## Seven day Average style\n",
    "PivotTable = pd.pivot_table(Corr_flow,values='Flow compound weir stormflow clipped (gpm)',columns=['Weekday'],index=['Hour'],aggfunc=np.mean)\n",
    "col_order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "PivotTable = PivotTable.reindex_axis(col_order,axis=1)\n",
    "PivotTable.to_excel(final_flow_ExcelFile,site_name+'PivotTable-Avg7day')\n",
    "## Format Pivot Table \n",
    "pivot = final_flow_ExcelFile.sheets[site_name+'PivotTable-Avg7day']\n",
    "## Conditional formatting\n",
    "# Add a format. Yellow fill with RED text.\n",
    "redtxt = final_flow_ExcelFile.book.add_format({'bg_color': '#FFFF00',\n",
    "                           'font_color': '#FF0000'})\n",
    "# Add a format. Yellow fill with black text.\n",
    "blacktxt = final_flow_ExcelFile.book.add_format({'bg_color': '#FFFF00',\n",
    "                           'font_color': '#000000'})\n",
    "day_cols={'Monday':'B','Tuesday':'C','Wednesday':'D','Thursday':'E','Friday':'F','Saturday':'G','Sunday':'H'}\n",
    "col_order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "for index, letter in enumerate(string.ascii_uppercase[1:9]):\n",
    "    ## Count cells over 25th percentile\n",
    "    pivot.write_formula(25,index, '=SUMPRODUCT(--('+letter+'2:'+letter+'25>PERCENTILE($B$2:$H$25,0.85)))')\n",
    "## Annotate\n",
    "pivot.write(25,0, 'Count>15% by day')\n",
    "pivot.write(26,3, 'Count>15% by day')\n",
    "for i, day in zip(np.arange(27,34,1),col_order):\n",
    "    col = day_cols[day]\n",
    "    print i, day, col        \n",
    "    pivot.write(i,0,day)\n",
    "    pivot.write_formula(i,1,'=AVERAGE('+col+'2:'+col+'25)')\n",
    "    pivot.write(i,2,'>Avg')\n",
    "    pivot.write_formula(i,3,'=SUM('+col+'26)')\n",
    "    ## Conditionally format each day\n",
    "    pivot.conditional_format(col+'2:'+col+'25', {'type': 'cell','criteria': '>=','value':'$B$35','format': redtxt})\n",
    "    pivot.conditional_format(col+'2:'+col+'25', {'type': 'cell','criteria': '>=','value':'$B$'+str(i+1),'format': blacktxt})\n",
    "pivot.write(34,0,'Top 15th%ile (excluding zeros)')\n",
    "pivot.write_formula(34,1,'=PERCENTILE(IF(B2:H25>0, B2:H25), 0.85)')\n",
    "pivot.write(34,2,'>15th%ile excl 0s')\n",
    "pivot.write(34,3,'(need to hit F2, then Ctrl+Shift+Enter to execute equation if you edit it)')\n",
    "pivot.write(35,0,'Top 15th%ile (including zeros)')\n",
    "pivot.write_formula(35,1,'=PERCENTILE(B2:H25,0.85)')\n",
    "pivot.write(35,2,'>15th%ile incl 0s')\n",
    "\n",
    "### SAVE FINAL FILE\n",
    "final_flow_ExcelFile.save()\n",
    "# Final Hydrograph    \n",
    "fig, ax1 = plt.subplots(1,1,figsize = (14,8))\n",
    "## FLOW\n",
    "ax1.plot_date(Corr_flow.index, Corr_flow['Flow compound weir (gpm)'], marker='None', ls='-', c='grey',alpha=0.2,label='Stormflow, clipped, compound weir')\n",
    "ax1.plot_date(Corr_flow.index, Corr_flow['Flow compound weir stormflow clipped (gpm)'], marker='None', ls='-', c='b',label='Flow, compound weir')\n",
    "## BASEFLOW\n",
    "ax1.plot_date(Corr_flow.index,Corr_flow['Baseflow (gpm)'], marker='None', ls='-', c='grey',label='Baseflow')\n",
    "## RAIN\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot_date(Rain1H.index, Rain1H['Rain_in'], marker='None',ls='steps-mid',color='skyblue',label='Precip: '+rain_gauge_site_list.ix[site_name]['rain_gauge_name'])\n",
    "## FORMAT\n",
    "ax1.set_ylim(-Corr_flow['Flow compound weir stormflow clipped (gpm)'].max() * 0.25, Corr_flow['Flow compound weir stormflow clipped (gpm)'].max() * 2.)\n",
    "ax2.set_ylim(0, Rain1H['Rain_in'].max() * 3.)\n",
    "ax2.invert_yaxis()\n",
    "## LEGEND\n",
    "ax1.legend(fontsize=12,loc='lower left'), ax2.legend(fontsize=12,loc='lower right')\n",
    "ax1.set_ylabel('Flow (gpm)'), ax2.set_ylabel('Precip (inches)')\n",
    "ax1.xaxis.set_major_formatter(mpl.dates.DateFormatter('%A \\n %m/%d/%y %H:%M'))\n",
    "plt.xticks(rotation=90)\n",
    "## set x-axis to monitoring period\n",
    "ax1.set_xlim(start_time_loc, end_time_loc)\n",
    "fig.suptitle('Working Draft Hydrograph for site: '+site_name,fontsize=16,fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "fig.savefig(maindir+'Flow_Output_Excel_files/Hydrographs/'+site_name+'-working hydrograph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
