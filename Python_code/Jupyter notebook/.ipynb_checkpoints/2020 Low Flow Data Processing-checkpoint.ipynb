{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the 2020 Low Flow Data Processing Script.\n",
    "***\n",
    "#### click in cell and then alt+Enter to execute\n",
    "#### Follow through the steps below to produce wonderfully accurate and high quality flow data\n",
    "#### Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jt -t onedork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Data Directories (will hopefully replace with a database soon)\n",
    "maindir = 'C:/Users/alex.messina/Documents/GitHub/2020_County_LowFlow/'\n",
    "import os\n",
    "os.chdir('C:/Users/alex.messina/Documents/GitHub/2020_County_LowFlow/Python_code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "## Import Standard modules\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import string\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import calendar\n",
    "from scipy import signal\n",
    "## Plotting modules\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "## Jupyter interactive plots\n",
    "import mpld3\n",
    "#mpld3.enable_notebook()\n",
    "## for Zentra API\n",
    "from pytz import timezone\n",
    "import json\n",
    "import urllib2\n",
    "## for OneRain data getter \n",
    "import requests\n",
    "import time\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "## Get google sheets\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from df2gspread import df2gspread as d2g \n",
    "## Image tools\n",
    "import matplotlib.image as mpimg\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "## Path to Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# Import Custom Modules\n",
    "from ZentraAPI import *\n",
    "from Get_GoogleSheets import *\n",
    "from OneRain_data import *\n",
    "from Excel_Plots import *\n",
    "from OvertoppingFlows import *\n",
    "from hover_points import *\n",
    "# make the screen bigger!\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(data=\"\"\" <style>    div#notebook-container    { width: 95%; }    div#menubar-container    \n",
    "{ width: 80%; }    div#maintoolbar-container { width: 99%; } </style> \"\"\"))       \n",
    "## Display outputs side by side\n",
    "CSS = \"\"\".output {flex-direction: row;}\"\"\"\n",
    "HTML('<style>{}</style>'.format(CSS))\n",
    "## Open HvF table - can be Google Sheets or straight from GitHub?\n",
    "#HvF = pd.read_csv(maindir+'Ancillary_files/HvF-90degweir.csv',index_col='Level (in)') # Local file\n",
    "#Hvf = open_HvF_90degweir()  # Google Sheets\n",
    "HvF = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/HvF-90degweir.csv',index_col=0) # GitHub\n",
    "## WEIR DIMENSIONS FOR OVERTOPPING FLOWS - can be Google Sheets or straight from GitHub?\n",
    "#weir_dims = pd.read_excel(maindir+'Ancillary_files/Weir Dims 2020.xlsx',sheetname='2020',index_col='Site',skiprows=1, parse_cols='A:I',na_values=['Not Applicable']) # Local file\n",
    "#weir_dims = open_weir_dims() # Google Sheets\n",
    "weir_dims = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Weir_Dims_2020.csv',index_col=0,na_values=['Not Applicable']) # GitHub\n",
    "\n",
    "print \"All modules imported. Let's go!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define site name and start/end times\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Update the Site Name and start/end times\n",
    "site_name = 'CAR-070'\n",
    "start_time_loc = dt.datetime(2020,5,1,0,0)\n",
    "end_time_loc = dt.datetime(2020,6,1,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water Level Data\n",
    "***\n",
    "#### from Local file or GitHub\n",
    "#### original data is batch downloaded from Zentra via the ZentraAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FROM CSV\n",
    "## If data is already downloaded, load from csv\n",
    "#WL = pd.DataFrame.from_csv(maindir+'Water_Level_data/'+site_name+'raw_data_ZentraAPI.csv') # Local file\n",
    "WL = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Water_Level_data/'+site_name+'_raw_data_ZentraAPI.csv') # GitHub\n",
    "## Inspect the data from Zentra to make sure it looks right...\n",
    "WL.ix[start_time_loc:end_time_loc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Rain data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Get the rain gauge used for the site\n",
    "#rain_gauge_site_list = pd.DataFrame.from_csv(maindir+'Ancillary_files/Rain_gauge_to_sites_list.csv') # Local File\n",
    "rain_gauge_site_list = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Rain_gauge_to_sites_list.csv',index_col=0) # GitHub\n",
    "rain_gauge_name = rain_gauge_site_list.ix[site_name]['rain_gauge_name']\n",
    "print 'Rain gauge used for '+site_name+' is '+ rain_gauge_name\n",
    "## Rain gauges indexed by rain gauge name\n",
    "#Rain_gauge_info = pd.DataFrame.from_csv(maindir+'Ancillary_files/Rain_gauge_info.csv') # Local File\n",
    "Rain_gauge_info = pd.read_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Ancillary_files/Rain_gauge_info.csv',index_col=0) # GitHub\n",
    "start_date, end_date = start_time_loc.strftime('%Y-%m-%d'), end_time_loc.strftime('%Y-%m-%d')\n",
    "#Rain1D = pd.DataFrame.from_csv(maindir+'Rain_data/'+rain_gauge_name+'_daily_'+start_date+'-'+end_date+'.csv') # Local file\n",
    "\n",
    "Rain1D = pd.DataFrame.from_csv('https://raw.githubusercontent.com/5025-Main/2020_County_LowFlow/master/Rain_data/'+rain_gauge_name.replace(' ','%20')+'_daily.csv') # GitHub\n",
    "## Check output \n",
    "Rain1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offsetting/Calibrating Water Level Data\n",
    "***\n",
    "1. Apply special offsets and clip bad data (storms will be clipped later)\n",
    "2. Calculate and apply offset from calibration points \n",
    "3. global offset to manually adjust\n",
    "#### >>> FINAL OFFSET saved to Google Sheets\n",
    "####  \n",
    "### 1a. Special Offsets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Add column of zero for data offset\n",
    "WL['spec_offset'] = 0.\n",
    "## Special Offsets and Bad Data Clips, Global Offset from Google Sheets\n",
    "spec_offsets  = open_2020_ClipsOffsets()[0]\n",
    "## Get offsets for each site\n",
    "offsets_list_for_site = spec_offsets[spec_offsets.index  == site_name]\n",
    "offsets_list_for_site\n",
    "## THIS IS AS TUPLES SO THE TUPLE IS INDEXED BY NUMBER NOT STRING\n",
    "for spec_offset in offsets_list_for_site.itertuples():\n",
    "    print ('Special offsets from Google sheet: ')\n",
    "    #print offset\n",
    "    ## set data in bad_data indices to nan\n",
    "    if pd.notnull(spec_offset.Start)==True and pd.notnull(spec_offset.End)==True:\n",
    "        print ('Special offset: '+spec_offset.Start.strftime('%m/%d/%y %H:%M')+' - '+spec_offset.End.strftime('%m/%d/%y %H:%M')+' = '+str(spec_offset.SpecialOffset_in)+ ' inches')\n",
    "        ## insert each offset value to Offset column\n",
    "        WL.loc[spec_offset.Start:spec_offset.End, ['spec_offset']] = spec_offset.SpecialOffset_in\n",
    "    else:\n",
    "        pass\n",
    "    print ('')   \n",
    "## Apply all offsets for unique shifts due to bad data or other issues\n",
    "WL['Level_spec_off'] = WL['in Water Level'] + WL['spec_offset']\n",
    "WL['Level_spec_off'] = WL['Level_spec_off'].round(2)\n",
    "WL[['in Water Level','spec_offset','Level_spec_off']].head()\n",
    "#WL[['in Water Level','Level_spec_off']].plot(figsize=(14,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Clip bad data (don't want it used with Field Measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips =  open_2020_ClipsOffsets()[2]\n",
    "\n",
    "try: # one entry dataframes are weird\n",
    "    clips_for_site = pd.DataFrame(clips.loc[site_name,:])\n",
    "    bad_data_clips = clips_for_site[clips_for_site['Reason']=='Invalid']\n",
    "except:\n",
    "    try:\n",
    "        clips_for_site = pd.DataFrame(clips.loc[site_name,:]).T # have to make DF and Transpose it \n",
    "        bad_data_clips = clips_for_site[clips_for_site['Reason']=='Invalid']   \n",
    "    except KeyError:\n",
    "        print 'No clips found'\n",
    "        bad_data_clips = pd.DataFrame()\n",
    "    \n",
    "## iterate over list of bad data and clip from 'offset_flow_clipped'....\n",
    "print ('Clipping bad/invalid data....')\n",
    "WL['Level_spec_off_clipbad'] = WL['Level_spec_off']\n",
    "for clip in bad_data_clips.iterrows():\n",
    "    clip_start, clip_end = clip[1]['Start'], clip[1]['End']\n",
    "    if pd.isnull(clip_start)==False and pd.isnull(clip_end) == False:\n",
    "        print ('Clipped Invalid data from: '+clip_start.strftime('%m/%d/%y %H:%M')+'-'+clip_end.strftime('%m/%d/%y %H:%M'))\n",
    "        ## set data in WL indices to nan\n",
    "        WL.loc[clip_start:clip_end, ['Level_spec_off_clipbad']] = np.nan\n",
    "    else:\n",
    "        print ('No data to clip...')\n",
    "        pass   \n",
    "#WL[['in Water Level','Level_spec_off','Level_spec_off_clipbad']].plot(figsize=(16,4))\n",
    "bad_data_clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate offset from calibration measurements\n",
    "### Field Data for Calibrations\n",
    "### Load from Google Sheets or csv if already saved\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Open FDS from Google Sheets\n",
    "fds = open_fds_from_google_sheet()\n",
    "fds['NOTES'] = fds['NOTES'].replace(np.nan, '', regex=True)\n",
    "## Save to csv\n",
    "fds.to_csv(maindir+'Ancillary_files/FDS/Field_data_sheet_backup.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from csv\n",
    "fds = pd.DataFrame.from_csv(maindir+'Ancillary_files/FDS/Field_data_sheet_backup.csv')\n",
    "## Round to 5Min\n",
    "fds['Datetime'] = pd.to_datetime(fds['Date and Time']).apply(lambda x: dt.datetime(x.year, x.month, x.day, x.hour,5*(x.minute // 5)))\n",
    "## Make Index line up with Excel row numbers for easy reference\n",
    "fds.index+=2\n",
    "## Drop any duplicate rows so it doesn't weight the average \n",
    "fds_len =  len(fds)\n",
    "fds = fds.drop_duplicates(keep='first')\n",
    "fds_len_no_dup = len(fds)\n",
    "diff = fds_len - fds_len_no_dup\n",
    "print ('')\n",
    "print ('Dropped '+str(diff)+' duplicate rows')\n",
    "print ('')\n",
    "## strip commas from volume measurement\n",
    "fds['Flow Measurement, Volume in mL'] = fds['Flow Measurement, Volume in mL'].apply(lambda x: x.replace(',',''))\n",
    "## cm to inches\n",
    "fds['Level_above_V_cm'] = fds['Height above (+) or below (-) v-notch in cm']\n",
    "fds['Level_above_V_in'] = np.round(fds['Level_above_V_cm'] / 2.54, 2)\n",
    "## Flow in cfs: mL to cfs divided by seconds\n",
    "fds['Flow_meas_gpm'] = (fds['Flow Measurement, Volume in mL'].astype('float') / fds['Flow Measurement, Time in Seconds '].astype('float')) * 0.02 #1mL per second is 0.02 gpm\n",
    "fds['Flow_meas_gpm'] = fds['Flow_meas_gpm'].round(3)\n",
    "## Display measurements\n",
    "fds[['Site ID','Datetime','Level_above_V_in','Flow_meas_gpm']].head()\n",
    "#fds[['Level_above_V_in','Flow_meas_gpm']].plot.scatter('Level_above_V_in','Flow_meas_gpm',figsize=(8,6))\n",
    "#fds[['Site ID','Datetime','Date and Time','Flow Condition',u'NOTES',u'Site Photo or Video?', u'Height above (+) or below (-) v-notch in cm',u'Flow Measurement, Time in Seconds ',u'Flow Measurement, Volume in mL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Offset from Calibration Points for Site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Choose time for selecting points\n",
    "cal_start, cal_end = start_time_loc, end_time_loc\n",
    "# FIELD MEASUREMENTS\n",
    "field_meas = fds[fds['Site ID']==site_name][['Datetime','Level_above_V_in','Flow_meas_gpm']]\n",
    "## Add PT level data to field measured level \n",
    "for t in field_meas['Datetime'].values:\n",
    "    t = pd.to_datetime(t)\n",
    "    #print ('Field measurement time:' + str(t))\n",
    "    try:\n",
    "        #print 'Level data from Meter: '\n",
    "        #print WL.loc[t]['Level_spec_off_clipbad'] ## water level after manual offset\n",
    "        field_meas.loc[field_meas['Datetime']==t, 'Level_spec_off_clipbad'] = WL.loc[t]['Level_spec_off_clipbad']\n",
    "    except:\n",
    "        try:\n",
    "            ' Shifting calibration time back 5 miutes....'\n",
    "            t = t - dt.timedelta(minutes=5)\n",
    "            field_meas.loc[field_meas['Datetime']==t, 'Level_in'] = WL.loc[t]['Level_in']\n",
    "        except:\n",
    "            pass\n",
    "## Add the flow that would be predicted from v-notch equation\n",
    "try: \n",
    "    field_meas.loc[:,'Predicted_flow'] =  [HvF.loc[np.round(x,2)]['Q (GPM)'] for x in field_meas['Level_above_V_in'].values]\n",
    "except KeyError:\n",
    "    #field_meas_level_nozeros = field_meas_level[field_meas_level['Level_above_V_in'] >=0.]\n",
    "    #field_meas_level_nozeros.loc['Predicted_flow'] = 0.\n",
    "    field_meas.loc[:,'Predicted_flow'] = 0.\n",
    "## Display field measurements of level and flow \n",
    "display(field_meas)\n",
    "## Calibration measurements made in May, QC measurements made June-Sept\n",
    "field_meas_QC = field_meas[field_meas['Datetime']>cal_end] ## after 5/31 should be only QC measurements\n",
    "field_meas_Cal = field_meas[(field_meas['Datetime']>=cal_start) & (field_meas['Datetime']<=cal_end)]\n",
    "\n",
    "## Calculate average offset from field data\n",
    "field_meas_Cal['calculated offset'] = field_meas_Cal['Level_above_V_in'] - field_meas_Cal['Level_spec_off_clipbad'] ## Measured - Manually adjusted PT reading\n",
    "## Calculate total offset  \n",
    "calculated_offset = field_meas_Cal['calculated offset'].mean()\n",
    "#print 'Calculated offset = '+str(calculated_offset)+' in'\n",
    "## Copy over data that has already had special offsets applied and bad data clipped\n",
    "## Apply calculated offset\n",
    "WL['Level_spec_off_clipbad_calc_off'] = WL['Level_spec_off_clipbad'] + calculated_offset\n",
    "## Display water level data\n",
    "fig, ax1 = plt.subplots(1,1,figsize=(14,4))\n",
    "ax1.plot(WL.index,WL['in Water Level'],marker='None',ls='-',c='grey',label='Raw Level data')\n",
    "ax1.plot(WL.index,WL['Level_spec_off'],marker='None',ls='-',c='red',label='Bad data clipped out')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad'],marker='None',ls='-',c='orange',label='Raw Level data + special offset')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad_calc_off'],marker='None',ls='-',c='green',label='Level offset by calibrations')\n",
    "## Plot field measurements\n",
    "from matplotlib.dates import DateFormatter, date2num\n",
    "ax1.plot_date(date2num(field_meas_Cal['Datetime'].values),field_meas_Cal['Level_above_V_in'].values,marker='s',c='b',label='Initial Calibration measurements')\n",
    "#ax1.plot(date2num(field_meas_QC['Datetime'].values),field_meas_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "ax1.axhline(0,c='k')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey',ls='--')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k',ls='--')\n",
    "ax1.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Global Offset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Get global offset (stored in Google Sheets)\n",
    "glob_offsets = open_2020_ClipsOffsets()[1]\n",
    "glob_offset = glob_offsets.ix[site_name]['GlobalOffset_in']\n",
    "print 'Global offset for '+site_name+' = '+str(glob_offset) +' in.'\n",
    "## Apply global offset\n",
    "WL['Level_spec_off_clipbad_calc_off_glob_off'] = WL['Level_spec_off_clipbad_calc_off'] + glob_offset\n",
    "## Final, offset and cleaned water level data for flow calculation\n",
    "WL['Level_in'] = WL['Level_spec_off_clipbad_calc_off_glob_off']\n",
    "## Highlight missing data  \n",
    "missing_data = pd.DataFrame(WL[np.isnan(WL['Level_in'])]['Level_in'])\n",
    "missing_data['missing_level_data'] = 0.\n",
    "missing_data = missing_data.reindex(index=pd.date_range(start_time_loc,end_time_loc,freq='5Min'))\n",
    "## Display water level data\n",
    "fig, ax1 = plt.subplots(1,1,figsize=(14,4))\n",
    "#ax1.plot(WL.index,WL['in Water Level'],marker='None',ls='-',c='grey',label='Raw Level data')\n",
    "#ax1.plot(WL.index,WL['Level_spec_off'],marker='None',ls='-',c='red',label='Bad data clipped out')\n",
    "#ax1.plot(WL.index,WL['Level_spec_off_clipbad'],marker='None',ls='-',c='orange',label='Raw Level data + special offset')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad_calc_off'],marker='None',ls='-',c='green',label='Level offset by calibrations')\n",
    "ax1.plot(WL.index,WL['Level_spec_off_clipbad_calc_off_glob_off'],marker='None',ls='-',c='blue',label='Final Level data, global offset:'+str(glob_offset) +' in.')\n",
    "## Plot field measurements\n",
    "from matplotlib.dates import DateFormatter, date2num\n",
    "ax1.plot_date(date2num(field_meas_Cal['Datetime'].values),field_meas_Cal['Level_above_V_in'].values,marker='s',c='k',label='Initial Calibration measurements')\n",
    "#ax1.plot(date2num(field_meas_QC['Datetime'].values),field_meas_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "ax1.axhline(0,c='k')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey',ls='--')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k',ls='--')\n",
    "ax1.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Total offset\n",
    "#### >>> Saved in Google Sheets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalOffsets = open_2020_FinalOffsets()\n",
    "FinalOffsets = FinalOffsets[['CalculatedOffset_in','GlobalOffset_in','FinalOffset_in']]\n",
    "FinalOffsets.loc[site_name,'CalculatedOffset_in'] = calculated_offset\n",
    "FinalOffsets.loc[site_name,'GlobalOffset_in'] = glob_offset\n",
    "FinalOffset = calculated_offset + glob_offset\n",
    "FinalOffsets.loc[site_name,'FinalOffset_in'] = FinalOffset\n",
    "save_df_to_GoogleSheets(FinalOffsets, worksheet_name='FinalOffsets',spreadsheet_key=\"1U0UnBJrpMNEtDYctO2GW0fuobdc8vJfdLIWbvSr--ss\")\n",
    "FinalOffsets.to_csv(maindir+'Ancillary_files/FinalOffsets_backup.csv')\n",
    "## Display\n",
    "FinalOffsets.ix[site_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Flow\n",
    "### HvF table for 90deg v-notch until water level exceeds v, then CTRSC equation\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## HvF table for 90 deg v-notch\n",
    "def level_to_gpm_vnotch(x):\n",
    "    if x < 0:\n",
    "        gpm = 0.0\n",
    "    else:\n",
    "        gpm = HvF.loc[np.round(x,2)]['Q (GPM)']\n",
    "    return gpm\n",
    "WL['Flow_gpm_v'] = WL['Level_in'].apply(lambda x: level_to_gpm_vnotch(x))    \n",
    "## Calculate flows when overtopping the weir\n",
    "WL['Flow_gpm'] = CTRSC_compound_weir(site_name, WL, weir_dims)#,  True, True)\n",
    "WL[['Level_in','Flow_gpm']].plot(figsize=(14,4),grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC Hydrograph and Clip storm flow\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clip storm flows\n",
    "clips =  open_2020_ClipsOffsets()[2] # reload every time so Google sheet can be updated\n",
    "try: # one entry dataframes are weird\n",
    "    clips_for_site = pd.DataFrame(clips.loc[site_name,:])\n",
    "    storm_clips = clips_for_site[clips_for_site['Reason']=='Storm']\n",
    "except:\n",
    "    try:\n",
    "        clips_for_site = pd.DataFrame(clips.loc[site_name,:]).T # have to make DF and Transpose it \n",
    "        storm_clips = clips_for_site[clips_for_site['Reason']=='Storm'] \n",
    "    except KeyError:\n",
    "        storm_clips = pd.DataFrame()\n",
    "## iterate over list of bad data and clip from 'Flow_gpm'....\n",
    "print ('Clipping stormflow data....')\n",
    "WL['Flow_gpm_storm_clipped'] = WL['Flow_gpm']\n",
    "for clip in storm_clips.iterrows():\n",
    "    clip_start, clip_end = clip[1]['Start'], clip[1]['End']\n",
    "    if pd.isnull(clip_start)==False and pd.isnull(clip_end) == False:\n",
    "        print ('Clipped storm data from: '+clip_start.strftime('%m/%d/%y %H:%M')+'-'+clip_end.strftime('%m/%d/%y %H:%M'))\n",
    "        ## set data in WL indices to nan\n",
    "        WL.loc[clip_start:clip_end, ['Flow_gpm_storm_clipped']] = np.nan\n",
    "    else:\n",
    "        print ('No data to clip...')\n",
    "        pass   \n",
    "storm_clips\n",
    "\n",
    "########################################\n",
    "##### QC Hydrograph ###################\n",
    "#####################################\n",
    "fig, (ax1, ax2, ax4) = plt.subplots(3,1,figsize=(14,7),sharex=True)\n",
    "## Plot full scale level data\n",
    "ax1.plot_date(missing_data.index,missing_data['missing_level_data'],marker='None',ls='-',c='r',label='Missing Data')\n",
    "## raw\n",
    "ax1.plot_date(WL.index, WL['in Water Level'], marker='None',ls='-',c='grey',alpha=0.25,label='Raw level data')\n",
    "##raw +final offset\n",
    "ax1.plot_date(WL.index, WL['Level_in'], marker='None',ls='-',c='g',alpha=0.25,label='Raw level + FinalOffset ('+\"%.2f\"%FinalOffset+' in.)')\n",
    "## Plot field measurements\n",
    "ax1.plot_date(field_meas_Cal['Datetime'].values,field_meas_Cal['Level_above_V_in'].values,marker='s',c='b',label='Initial Calibration measurements')\n",
    "ax1.plot_date(field_meas_QC['Datetime'].values,field_meas_QC['Level_above_V_in'].values,marker='s',c='r',label='Follow-up QC measurements')\n",
    "## Plot maximum v-notch height\n",
    "ax1.axhline(weir_dims.loc[site_name,'h2'],color='grey')\n",
    "ax1.axhline(weir_dims.loc[site_name,'h1'] + weir_dims.loc[site_name,'h2'],color='k')\n",
    "textstr = 'Weir crest height: '+str(weir_dims.loc[site_name,'h2'])+' inches'\n",
    "#ax1.annotate(textstr, (mpl.dates.date2num(pd.to_datetime(weir_dims.loc['CAR-007','Measurement date'])),weir_dims.loc[site_name,'h2']))\n",
    "## Plot temp\n",
    "#ax1_1 = ax1.twinx()\n",
    "#ax1_1.plot_date(WL.index, WL['Temp_F'], marker='None',ls='-',c='grey',label='Temp F')\n",
    "\n",
    "## Plot full scale flow data or\n",
    "## Conductivity data if available\n",
    "if u'mS/cm EC' in WL.columns:\n",
    "    ax2.plot_date(WL.index, WL[u'mS/cm EC'], marker='None',ls='-',c='orange',label= 'mS/cm EC')\n",
    "    ax2.set_ylabel('Sp.Cond (mS/cm)',color='orange'), \n",
    "else:\n",
    "    ax2.plot_date(WL.index, WL['Flow_gpm'], marker='None',ls='-',c='teal',label='Flow (gpm)')\n",
    "    ax2.set_ylabel('Flow (gpm)',color='b'), \n",
    "## Put notes on the plot\n",
    "for row in fds[fds['Site ID']==site_name][['Datetime','NOTES']].iterrows():\n",
    "    note = '\\n'.join(textwrap.wrap(str(row[1]['NOTES']), 16))\n",
    "    ax2.annotate(note,xy=(pd.to_datetime(row[1]['Datetime']),WL['Flow_gpm'].mean()),rotation=90,verticalalignment='bottom')\n",
    "    ax2.axvline(pd.to_datetime(row[1]['Datetime']),color='grey',alpha=0.5)\n",
    "### Plot precip on inverted, secondary y axis\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot_date(Rain1D.index, Rain1D['Rain_in'], marker='None',ls='steps-mid',color='teal',label='Precip: '+rain_gauge_name)\n",
    "## Plot flow data, compound weir\n",
    "ax4.plot_date(WL.index, WL['Flow_gpm'], marker='None',ls='-',c='grey',alpha=0.5,label='Flow (gpm)')\n",
    "## Plot flow data, storms clipped\n",
    "ax4.plot_date(WL.index, WL['Flow_gpm_storm_clipped'], marker='None',ls='-',c='green',label='Flow(gpm), storms clipped')\n",
    "## Plot calibration field measurements\n",
    "ax4.plot_date(field_meas_Cal['Datetime'].values,field_meas_Cal['Flow_meas_gpm'].values,marker='o',c='b',label='Initial Calibration measurements')\n",
    "## Plot QC field measurements\n",
    "ax4.plot_date(field_meas_QC['Datetime'].values,field_meas_QC['Flow_meas_gpm'].values,marker='o',c='r',label='Follow-up QC measurements')\n",
    "\n",
    "## Previous deliverable data\n",
    "#ax4.plot_date(del_df.index,del_df['Flow compound weir (gpm)'], marker='None',ls='-',c='b',label='Previous deliverable')\n",
    "\n",
    "### Plot precip on inverted, secondary y axis\n",
    "ax4_2 = ax4.twinx()\n",
    "ax4_2.plot_date(Rain1D.index, Rain1D['Rain_in'], marker='None',ls='steps-mid',color='teal',label='Precip: '+rain_gauge_name)\n",
    "\n",
    "## Format/set limits\n",
    "## full scale flow\n",
    "ax1.set_ylim(-3, WL['Level_in'].max() * 1.1)\n",
    "#ax2.set_ylim(-WL['offset_flow'].max() * 0.5, WL['offset_flow'].max() * 2.)\n",
    "ax3.set_ylim(0, Rain1D['Rain_in'].max() * 2.)\n",
    "ax4_2.set_ylim(0, Rain1D['Rain_in'].max() * 3.)\n",
    "ax3.invert_yaxis(), ax4_2.invert_yaxis()\n",
    "\n",
    "## low flow\n",
    "ax4.set_ylim(-WL['Flow_gpm_storm_clipped'].max() * 0.45, WL['Flow_gpm_storm_clipped'].max() * 1.2)\n",
    "## set x-axis to monitoring period\n",
    "ax1.set_xlim(start_time_loc, end_time_loc)\n",
    "ax1.grid(True)\n",
    "ax1.set_ylabel('Level (inches)',color='g')\n",
    "ax3.set_ylabel('Precip (inches)',color='teal')\n",
    "ax4.set_ylabel('Flow (gpm)',color='b')\n",
    "\n",
    "ax1.legend(fontsize=6,numpoints=1,ncol=2,loc='upper left')\n",
    "ax2.legend(fontsize=6,loc='lower left'), ax3.legend(fontsize=12,loc='lower right')\n",
    "ax4.legend(fontsize=6,numpoints=1,ncol=5,loc='lower left')\n",
    "\n",
    "ax4.xaxis.set_major_formatter(mpl.dates.DateFormatter('%A \\n %m/%d/%y %H:%M'))\n",
    "\n",
    "fig.suptitle('Data processing for site: '+site_name,fontsize=12,fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### QC SCATTERPLOTS\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(16,5))\n",
    "fig.suptitle(site_name,fontweight='bold',fontsize=8)\n",
    "\n",
    "## MEASURED LEVEL VS OFFSET LEVEL\n",
    "field_meas_Cal['Final Level Data (in)'] =  field_meas_Cal['Datetime'].apply(lambda x: WL.loc[x]['Level_in'])\n",
    "field_meas_Cal['Level_diff_in'] = field_meas_Cal['Level_above_V_in'] - field_meas_Cal['Final Level Data (in)'] \n",
    "if len(field_meas_QC)>0:\n",
    "    field_meas_QC['Final Level Data (in)'] =  field_meas_QC['Datetime'].apply(lambda x: WL.loc[x]['Level_in'])\n",
    "    field_meas_QC['Level_diff_in'] = field_meas_QC['Level_above_V_in'] - field_meas_QC['Final Level Data (in)'] \n",
    "one_to_one = ax1.plot([-10,1000],[-10,1000],ls='-',marker='None',color='grey',alpha=0.5,label='1:1')\n",
    "one_to_one_plus = ax1.plot([-10,1000],[-10.1,1000.1],ls='--',marker='None',color='grey',alpha=0.5,label='+0.1')\n",
    "one_to_one = ax1.plot([-10,1000],[-9.9,999.9],ls='--',marker='None',color='grey',alpha=0.5,label='-0.1')\n",
    "Calpoints = ax1.plot(field_meas_Cal['Level_above_V_in'],field_meas_Cal['Final Level Data (in)'],ls='None',marker='o',c='b',markersize=12,label='Initial Calibration measurements')\n",
    "#QCpoints = ax1.plot(field_meas_QC['Level_above_V_in'],field_meas_QC['Final Level Data (in)'],ls='None',marker='o',c='r',markersize=12,label='Follow-up QC measurements')   \n",
    "ax1.set_xlabel('Measured Level (in)',fontweight='bold',fontsize=10)\n",
    "ax1.set_ylabel('Final Level data (in)',fontweight='bold',fontsize=10)\n",
    "max_level = field_meas_Cal[[u'Level_above_V_in', u'Final Level Data (in)']].max().max()\n",
    "ax1.set_xlim(0, 1.3 *max_level.max())\n",
    "ax1.set_ylim(0, 1.3 *max_level.max())\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_title('Measured level vs Offset Level',fontweight='bold',fontsize=12)\n",
    "ax1.grid(True)\n",
    "hover_points(Calpoints, list(field_meas_Cal['Datetime']), fig, ax1)\n",
    "#hover_points(QCpoints, list(field_meas_QC['Datetime']), fig, ax1)\n",
    "\n",
    "## MEASURED FLOW VS FINAL FLOW\n",
    "field_meas_Cal['Final Flow Data (gpm)'] =  field_meas_Cal['Datetime'].apply(lambda x: WL.loc[x]['Flow_gpm'])\n",
    "field_meas_QC['Final Flow Data (gpm)'] =  field_meas_QC['Datetime'].apply(lambda x: WL.loc[x]['Flow_gpm'])\n",
    "\n",
    "## PREDICTED FLOW VS MEASURED FLOW\n",
    "one_to_one = ax2.plot([0,1000],[0,1000],ls='-',marker='None',color='grey',alpha=0.5,label='1:1')\n",
    "Calpoints = ax2.plot(field_meas_Cal['Final Flow Data (gpm)'],field_meas_Cal['Predicted_flow'],ls='None',marker='o',markersize=12,label='Initial Calibration measurements',c='b')\n",
    "QCpoints = ax2.plot(field_meas_QC['Final Flow Data (gpm)'],field_meas_QC['Predicted_flow'],ls='None',marker='o',markersize=12,label='Follow-up QC measurements',c='r')\n",
    "ax2.set_xlabel('Measured Flow (gpm)',fontweight='bold',fontsize=10)\n",
    "ax2.set_ylabel('Predicted flow from Measured Level (gpm)',fontweight='bold',fontsize=10)\n",
    "ax2.set_xlim(0, 1.3 *field_meas_Cal['Predicted_flow'].max())\n",
    "ax2.set_ylim(0, 1.3 *field_meas_Cal['Predicted_flow'].max())\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_title('Measured flow vs Predicted flow from measured level in v-notch',fontweight='bold',fontsize=10)\n",
    "hover_points(Calpoints, list(field_meas_Cal['Datetime']), fig, ax2)\n",
    "hover_points(QCpoints, list(field_meas_QC['Datetime']), fig, ax2)\n",
    "ax2.grid(True)\n",
    "\n",
    "\n",
    "## Calculate flow differences in gpm and %\n",
    "field_meas_Cal['difference_gpm'] = field_meas_Cal['Final Flow Data (gpm)'] - field_meas_Cal['Flow_meas_gpm']\n",
    "field_meas_Cal['difference_%'] = (abs(field_meas_Cal['Final Flow Data (gpm)'] - field_meas_Cal['Flow_meas_gpm']) / field_meas_Cal['Flow_meas_gpm']) * 100.\n",
    "print 'Average difference between measured, and final processed flow values: '+\"%.3f\"%field_meas_Cal['difference_gpm'].mean() +' gpm'\n",
    "print 'Average difference between measured, and final processed flow values: '+\"%.1f\"%field_meas_Cal['difference_%'].mean() +' %'\n",
    "one_to_one = ax3.plot([0,1000],[0,1000],ls='-',marker='None',color='grey',alpha=0.5,label='1:1')\n",
    "Calpoints = ax3.plot(field_meas_Cal['Flow_meas_gpm'],field_meas_Cal['Final Flow Data (gpm)'],ls='None',marker='o',markersize=12,label='Initial Calibration measurements',c='b')\n",
    "QCpoints = ax3.plot(field_meas_QC['Flow_meas_gpm'],field_meas_QC['Final Flow Data (gpm)'],ls='None',marker='o',markersize=12,label='Follow-up QC measurements',c='r')\n",
    "ax3.set_xlabel('Measured Flow (gpm)',fontweight='bold',fontsize=10)\n",
    "ax3.set_ylabel('Flow data Output (gpm)',fontweight='bold',fontsize=10)\n",
    "ax3.set_xlim(0, 1.3 *field_meas_Cal[['Final Flow Data (gpm)','Flow_meas_gpm']].max().max())\n",
    "ax3.set_ylim(0, 1.3 *field_meas_Cal[['Final Flow Data (gpm)','Flow_meas_gpm']].max().max())\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.set_title('Measured flow vs Flow data output',fontweight='bold',fontsize=12)\n",
    "hover_points(Calpoints, list(field_meas_Cal['Datetime']),fig, ax3)\n",
    "hover_points(QCpoints, list(field_meas_QC['Datetime']),fig, ax3)\n",
    "ax3.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.90)\n",
    "\n",
    "display(field_meas_Cal[[u'Datetime', u'Level_above_V_in', u'Final Level Data (in)','Level_diff_in']])\n",
    "display(field_meas_Cal[['Datetime','Flow_meas_gpm','Final Flow Data (gpm)','difference_gpm','difference_%']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrograph Separation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.990\n",
    "\n",
    "flow_df = WL[[u'Flow_gpm']]\n",
    "## gap fill\n",
    "flow_df = flow_df.fillna(flow_df.interpolate(method='linear')).fillna(flow_df.mode().loc[0].values[0])\n",
    "#flow_df[''Flow_compound_weir'].plot(c='b')\n",
    "## Function to add original flow peaks back into dataset\n",
    "def peaks(original_flow, smoothed, peak_val=2):\n",
    "    if abs(original_flow - smoothed) > peak_val:\n",
    "        flow = original_flow\n",
    "    else:\n",
    "        flow = smoothed\n",
    "    return flow\n",
    "## Smoothing\n",
    "flow_df['rolling'] = flow_df['Flow_gpm'].rolling(12,min_periods=3,center=True).mean()\n",
    "#flow_df['rolling'].plot(c='g') \n",
    "## Add peaks back into rolling data\n",
    "flow_df['rolling+peaks']  = flow_df.apply(lambda x: peaks(x['Flow_gpm'],x['rolling']), axis=1)\n",
    "#flow_df['rolling+peaks'].plot(c='r')\n",
    "## Butter filter\n",
    "b, a = signal.butter(3, 0.2, btype='lowpass', analog=False) ## 0.2 parameter selected by trial and error\n",
    "flow_df['butter'] = signal.filtfilt(b, a, flow_df['rolling+peaks'])\n",
    "#flow_df['butter'].plot(c='orange')\n",
    "flow_df['butter+peaks']  = flow_df.apply(lambda x: peaks(x['Flow_gpm'],x['butter'], 1.), axis=1)\n",
    "## CHoose a smoothed dataset to apply the DF to\n",
    "flow_df['Flow (gpm) smooth'] = flow_df['butter+peaks']\n",
    "## Set arbitrary index\n",
    "flow_df = flow_df.reset_index()\n",
    "## Baseflow df    \n",
    "df = flow_df\n",
    "## Define h_k1 (original flow data series\n",
    "df['h_k1'] = df['Flow (gpm) smooth']\n",
    "### BACKWARD FILTER\n",
    "## Fill in first value for q_k-1\n",
    "df.loc[0,'q_k-1'] = df.loc[0,'h_k1']\n",
    "## q_k-1\n",
    "for i in range(1,len(df)):\n",
    "    # (0.925 * q_k-1) + (((1+0.925)/2) * (q_k - q_k-1))\n",
    "    df.loc[i,'q_k-1'] = (alpha*df.loc[i-1,'q_k-1']) + (((1.+alpha)/2.) * (df.loc[i,'h_k1'] - df.loc[i-1,'h_k1']))\n",
    "## Change negatives to 0's\n",
    "df['q_k-1>0'] = df['q_k-1'].where(df['q_k-1']>0., 0.)\n",
    "## b_k1\n",
    "df['b_k1'] = df['h_k1'] - df['q_k-1>0']\n",
    "## FORWARD FILTER\n",
    "df['h_k2'] = df['h_k1'] - df['b_k1']\n",
    "## Fill in first value for q_k+1\n",
    "df.loc[df.index[-1],'q_k+1'] = 0.\n",
    "##q_k+1\n",
    "for i in range(df.index[-2],0,-1): ## iterate backwards\n",
    "    # (0.925 * q_k+1) + (((1+0.925)/2) * (q_k - q_k-1))\n",
    "    df.loc[i,'q_k+1'] = (alpha*df.loc[i+1,'q_k+1']) + (((1.+alpha)/2.) * (df.loc[i,'b_k1'] - df.loc[i+1,'b_k1']))\n",
    "## change negatives to 0's\n",
    "df['q_k+1>0'] = df['q_k+1'].where(df['q_k+1']>=0, 0.)\n",
    "## b_k2\n",
    "df['b_k2'] = df['b_k1'] - df['q_k+1>0']\n",
    "\n",
    "## Rest values back to date\n",
    "df = df.set_index(df['index'])\n",
    "## Deliver\n",
    "df[['h_k1','b_k1','b_k2']]\n",
    "\n",
    "flowoutput = df[['Flow_gpm','Flow (gpm) smooth']]\n",
    "flowoutput.loc[:,'Baseflow (gpm)'] = df['b_k2']\n",
    "flowoutput.loc[:,'Quickflow (gpm)'] = df['Flow (gpm) smooth'] - df['b_k2']\n",
    "\n",
    "## Put in original flow data and Mask where Nan values in orginal dataset\n",
    "flowoutput.loc[:,'Flow_gpm'] = WL[[u'Flow_gpm']]\n",
    "m = pd.notnull(flowoutput['Flow_gpm'])\n",
    "flowoutput = flowoutput.where(m, np.nan)   \n",
    "\n",
    "WL.loc[:,'Baseflow (gpm)'] = flowoutput['Baseflow (gpm)'].round(3)\n",
    "WL.loc[:,'Quickflow (gpm)'] = flowoutput['Quickflow (gpm)'].round(3)\n",
    "\n",
    "## Drop data for data dropouts\n",
    "WL['Baseflow (gpm)'] = np.where(WL['Level_in'].isnull(),np.nan,WL['Baseflow (gpm)'])\n",
    "WL['Baseflow (gpm)'] = np.where(WL['Level_in']<0.,0.,WL['Baseflow (gpm)']) ## Get rid of negative values\n",
    "WL['Quickflow (gpm)'] = np.where(WL['Level_in'].isnull(),np.nan,WL['Quickflow (gpm)'])\n",
    "\n",
    "#%% BASEFLOW PLOT\n",
    "## PLOT\n",
    "#    fig, (ax1,ax2,ax3) = plt.subplots(3,1,figsize=(20,10),sharex=True)\n",
    "fig, ax1 = plt.subplots(1,1,figsize=(14,6))\n",
    "fig.suptitle(site_name,fontsize=14, fontweight='bold')\n",
    "\n",
    "## Flow\n",
    "ax1.plot_date(WL.index, WL['Flow_gpm'],marker='None',ls='-',label='Orig. Flow Data (gpm)',c='grey')\n",
    "ax1.plot_date(flowoutput.index, flowoutput['Flow (gpm) smooth'],marker='None',ls='-',label='Smoothed Flow Data (gpm)',c='b')\n",
    "#ax1.plot_date(WL.index, WL['Baseflow (gpm)'] + WL['Quickflow (gpm)'],marker='None',ls='-',label='Quickflow (digital filter='+str(alpha)+')',c='r')\n",
    "ax1.plot_date(WL.index, WL['Baseflow (gpm)'],marker='None',ls='-',label='Baseflow (digital filter='+str(alpha)+')',c='g')\n",
    "ax1.set_ylabel('Flow (gpm)',fontweight='bold',fontsize=14)\n",
    "for ax in fig.axes:\n",
    "    ax.legend(loc='upper left')\n",
    "ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%m-%d %H:%M'))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95,hspace=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE TO EXCEL\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xl_columnrow(col,row=''):\n",
    "    \"\"\" Convert given row and column number to an Excel-style cell name. \"\"\"\n",
    "    LETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    result = []\n",
    "    while col:\n",
    "        col, rem = divmod(col-1, 26)\n",
    "        result[:0] = LETTERS[rem]\n",
    "    return ''.join(result)+str(row)\n",
    "\n",
    "## Get rid of all data for data dropouts\n",
    "for col in WL.columns:\n",
    "    #print col\n",
    "    try:\n",
    "        WL[col] = np.where(WL['Level_in'].isnull(),np.nan,WL[col])\n",
    "    except:\n",
    "        print 'skipped col: '+col\n",
    "        pass\n",
    "\n",
    "### FINALIZED FLOW OUTPUT\n",
    "## FLOW\n",
    "Corr_flow = WL[['Flow_gpm', 'Flow_gpm_storm_clipped']].round(3)\n",
    "Corr_flow.columns = ['Flow compound weir (gpm)', 'Flow compound weir stormflow clipped (gpm)']\n",
    "## Add base/quickflow\n",
    "Corr_flow[['Baseflow (gpm)','Quickflow (gpm)']] = WL[['Baseflow (gpm)','Quickflow (gpm)']]\n",
    "## Add temp and conductivity to deliverable\n",
    "Corr_flow[u'uS/cm EC'] = np.round(WL[u'mS/cm EC'] * 1000., 0)\n",
    "temp_col_name = [col for col in WL.columns if 'Water Temperature' in col]\n",
    "Corr_flow[u'\\xb0F Water Temperature'] = WL[temp_col_name].round(1)\n",
    "\n",
    "## PIVOT TABLE STUFF\n",
    "Corr_flow.loc[:,('Year')] = Corr_flow.index.year\n",
    "Corr_flow.loc[:,('Month')] = Corr_flow.index.month\n",
    "Corr_flow.loc[:,('Day')] = Corr_flow.index.day\n",
    "Corr_flow.loc[:,('Hour')] = Corr_flow.index.hour\n",
    "Corr_flow.loc[:,('Minute')] = Corr_flow.index.minute\n",
    "Corr_flow.loc[:,('Weekday')] = Corr_flow.index.map(lambda x: calendar.day_name[x.weekday()])\n",
    "\n",
    "## Kick out to Excel\n",
    "final_flow_ExcelFile = pd.ExcelWriter(maindir+'Flow_Output_Excel_files/'+site_name+'-working draft.xlsx')\n",
    "max_row, rain_max_row = Excel_Plots(site_name, Corr_flow, Rain1D, final_flow_ExcelFile, start_time_loc, end_time_loc)\n",
    "\n",
    "### Pivot TABLES\n",
    "## Old style-SUM but ADDING the multiplication by 5min (gpm->gp5M)\n",
    "PivotTable_Sum = pd.pivot_table(Corr_flow,values='Flow compound weir stormflow clipped (gpm)', columns=['Month','Day','Weekday'], index=['Hour'], aggfunc=np.sum).round(1) * 5. # *5 for 5Min interval data\n",
    "PivotTable_Sum.to_excel(final_flow_ExcelFile,site_name+'PivotTable-Sum')\n",
    "## Freeze Panes\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Sum'].freeze_panes(4, 1)\n",
    "## Conditional Formatting\n",
    "def rgb_hex(red,green,blue):\n",
    "    return '#%02x%02x%02x' % (red, green, blue)\n",
    "green, yellow, red = rgb_hex(99,190,123),rgb_hex(255,235,132),rgb_hex(248,105,107)\n",
    "max_col_row = xl_columnrow(len(PivotTable_Sum.columns)+1,28) #24th hour is on row 28\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Sum'].conditional_format('B5:'+max_col_row, {'type': '3_color_scale','min_color': green,'mid_color':yellow,'max_color':red})\n",
    "## Old style-AVG\n",
    "PivotTable_Avg = pd.pivot_table(Corr_flow,values='Flow compound weir stormflow clipped (gpm)', columns=['Month','Day','Weekday'], index=['Hour'], aggfunc=np.mean).round(3)\n",
    "PivotTable_Avg.to_excel(final_flow_ExcelFile,site_name+'PivotTable-Avg')\n",
    "## Freeze Panes\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Avg'].freeze_panes(4, 1)\n",
    "## Conditional Formatting\n",
    "max_col_row = xl_columnrow(len(PivotTable_Sum.columns)+1,28)  #24th hour is on row 28\n",
    "final_flow_ExcelFile.sheets[site_name+'PivotTable-Avg'].conditional_format('B5:'+max_col_row, {'type': '3_color_scale','min_color': green,'mid_color': yellow,'max_color': red})\n",
    "## Seven day Average style\n",
    "PivotTable = pd.pivot_table(Corr_flow,values='Flow compound weir stormflow clipped (gpm)',columns=['Weekday'],index=['Hour'],aggfunc=np.mean)\n",
    "col_order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "PivotTable = PivotTable.reindex_axis(col_order,axis=1)\n",
    "PivotTable.to_excel(final_flow_ExcelFile,site_name+'PivotTable-Avg7day')\n",
    "## Format Pivot Table \n",
    "pivot = final_flow_ExcelFile.sheets[site_name+'PivotTable-Avg7day']\n",
    "## Conditional formatting\n",
    "# Add a format. Yellow fill with RED text.\n",
    "redtxt = final_flow_ExcelFile.book.add_format({'bg_color': '#FFFF00',\n",
    "                           'font_color': '#FF0000'})\n",
    "# Add a format. Yellow fill with black text.\n",
    "blacktxt = final_flow_ExcelFile.book.add_format({'bg_color': '#FFFF00',\n",
    "                           'font_color': '#000000'})\n",
    "day_cols={'Monday':'B','Tuesday':'C','Wednesday':'D','Thursday':'E','Friday':'F','Saturday':'G','Sunday':'H'}\n",
    "col_order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "for index, letter in enumerate(string.ascii_uppercase[1:9]):\n",
    "    ## Count cells over 25th percentile\n",
    "    pivot.write_formula(25,index, '=SUMPRODUCT(--('+letter+'2:'+letter+'25>PERCENTILE($B$2:$H$25,0.85)))')\n",
    "## Annotate\n",
    "pivot.write(25,0, 'Count>15% by day')\n",
    "pivot.write(26,3, 'Count>15% by day')\n",
    "for i, day in zip(np.arange(27,34,1),col_order):\n",
    "    col = day_cols[day]\n",
    "    print i, day, col        \n",
    "    pivot.write(i,0,day)\n",
    "    pivot.write_formula(i,1,'=AVERAGE('+col+'2:'+col+'25)')\n",
    "    pivot.write(i,2,'>Avg')\n",
    "    pivot.write_formula(i,3,'=SUM('+col+'26)')\n",
    "    ## Conditionally format each day\n",
    "    pivot.conditional_format(col+'2:'+col+'25', {'type': 'cell','criteria': '>=','value':'$B$35','format': redtxt})\n",
    "    pivot.conditional_format(col+'2:'+col+'25', {'type': 'cell','criteria': '>=','value':'$B$'+str(i+1),'format': blacktxt})\n",
    "pivot.write(34,0,'Top 15th%ile (excluding zeros)')\n",
    "pivot.write_formula(34,1,'=PERCENTILE(IF(B2:H25>0, B2:H25), 0.85)')\n",
    "pivot.write(34,2,'>15th%ile excl 0s')\n",
    "pivot.write(34,3,'(need to hit F2, then Ctrl+Shift+Enter to execute equation if you edit it)')\n",
    "pivot.write(35,0,'Top 15th%ile (including zeros)')\n",
    "pivot.write_formula(35,1,'=PERCENTILE(B2:H25,0.85)')\n",
    "pivot.write(35,2,'>15th%ile incl 0s')\n",
    "\n",
    "### SAVE FINAL FILE\n",
    "final_flow_ExcelFile.save()\n",
    "# Final Hydrograph    \n",
    "fig, ax1 = plt.subplots(1,1,figsize = (14,8))\n",
    "## FLOW\n",
    "ax1.plot_date(Corr_flow.index, Corr_flow['Flow compound weir (gpm)'], marker='None', ls='-', c='grey',alpha=0.2,label='Stormflow, clipped, compound weir')\n",
    "ax1.plot_date(Corr_flow.index, Corr_flow['Flow compound weir stormflow clipped (gpm)'], marker='None', ls='-', c='b',label='Flow, compound weir')\n",
    "## BASEFLOW\n",
    "ax1.plot_date(Corr_flow.index,Corr_flow['Baseflow (gpm)'], marker='None', ls='-', c='grey',label='Baseflow')\n",
    "## RAIN\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot_date(Rain1D.index, Rain1D['Rain_in'], marker='None',ls='steps-mid',color='skyblue',label='Precip: '+rain_gauge_site_list.ix[site_name]['rain_gauge_name'])\n",
    "## FORMAT\n",
    "ax1.set_ylim(-Corr_flow['Flow compound weir stormflow clipped (gpm)'].max() * 0.25, Corr_flow['Flow compound weir stormflow clipped (gpm)'].max() * 2.)\n",
    "ax2.set_ylim(0, Rain1D['Rain_in'].max() * 3.)\n",
    "ax2.invert_yaxis()\n",
    "## LEGEND\n",
    "ax1.legend(fontsize=12,loc='lower left'), ax2.legend(fontsize=12,loc='lower right')\n",
    "ax1.set_ylabel('Flow (gpm)'), ax2.set_ylabel('Precip (inches)')\n",
    "ax1.xaxis.set_major_formatter(mpl.dates.DateFormatter('%A \\n %m/%d/%y %H:%M'))\n",
    "plt.xticks(rotation=90)\n",
    "## set x-axis to monitoring period\n",
    "ax1.set_xlim(start_time_loc, end_time_loc)\n",
    "fig.suptitle('Working Draft Hydrograph for site: '+site_name,fontsize=16,fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.95)\n",
    "fig.savefig(maindir+'Flow_Output_Excel_files/Hydrographs/'+site_name+'-working hydrograph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
